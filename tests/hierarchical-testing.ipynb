{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyrad.assemble.clustmap import *\n",
    "from ipyrad.assemble.cluster_across import *\n",
    "import ipyparallel as ipp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Assembly: 4-refpairtest\n",
      "from saved path: ~/Documents/ipyrad/tests/4-refpairtest.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deren/miniconda3/lib/python3.6/site-packages/ipyparallel/client/client.py:458: RuntimeWarning: \n",
      "            Controller appears to be listening on localhost, but not on this machine.\n",
      "            If this is true, you should specify Client(...,sshserver='you@oud')\n",
      "            or instruct your controller to listen on an external IP.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "data = ip.load_json(\"4-refpairtest.json\")\n",
    "samples = list(data.samples.values())\n",
    "sample = samples[0]\n",
    "force = True\n",
    "ipyclient = ipp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3J_0': <ipyrad.core.sample.Sample at 0x7f5b79e75da0>,\n",
       " '1D_0': <ipyrad.core.sample.Sample at 0x7f5b79c64f60>,\n",
       " '2H_0': <ipyrad.core.sample.Sample at 0x7f5b79c53a90>,\n",
       " '1B_0': <ipyrad.core.sample.Sample at 0x7f5b79c89630>,\n",
       " '2F_0': <ipyrad.core.sample.Sample at 0x7f5b79c89b38>,\n",
       " '3L_0': <ipyrad.core.sample.Sample at 0x7f5b79c89fd0>,\n",
       " '2E_0': <ipyrad.core.sample.Sample at 0x7f5b79c53c88>,\n",
       " '3I_0': <ipyrad.core.sample.Sample at 0x7f5b79c89128>,\n",
       " '3K_0': <ipyrad.core.sample.Sample at 0x7f5b79c175c0>,\n",
       " '2G_0': <ipyrad.core.sample.Sample at 0x7f5b79c17f28>,\n",
       " '1A_0': <ipyrad.core.sample.Sample at 0x7f5b79c17eb8>,\n",
       " '1C_0': <ipyrad.core.sample.Sample at 0x7f5b79c22978>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': (3, ['1A_0', '1B_0', '1C_0', '1D_0']),\n",
       " '2': (3, ['2E_0', '2F_0', '2G_0', '2H_0']),\n",
       " '3': (3, ['3I_0', '3J_0', '3K_0', '3L_0'])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popmins = {'1': 3, '2': 3, '3': 3}\n",
    "popdict = {\n",
    "    '1': ['1A_0', '1B_0', '1C_0', '1D_0'], \n",
    "    '2': ['2E_0', '2F_0', '2G_0', '2H_0'],\n",
    "    '3': ['3I_0', '3J_0', '3K_0', '3L_0'],\n",
    "}\n",
    "\n",
    "data._link_populations(popdict, popmins)\n",
    "data.populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ipyrad.core.sample.Sample at 0x7f5b79e75da0>,\n",
       " <ipyrad.core.sample.Sample at 0x7f5b79c64f60>,\n",
       " <ipyrad.core.sample.Sample at 0x7f5b79c53a90>,\n",
       " <ipyrad.core.sample.Sample at 0x7f5b79c89630>,\n",
       " <ipyrad.core.sample.Sample at 0x7f5b79c89b38>,\n",
       " <ipyrad.core.sample.Sample at 0x7f5b79c89fd0>,\n",
       " <ipyrad.core.sample.Sample at 0x7f5b79c53c88>,\n",
       " <ipyrad.core.sample.Sample at 0x7f5b79c89128>,\n",
       " <ipyrad.core.sample.Sample at 0x7f5b79c175c0>,\n",
       " <ipyrad.core.sample.Sample at 0x7f5b79c17f28>,\n",
       " <ipyrad.core.sample.Sample at 0x7f5b79c17eb8>,\n",
       " <ipyrad.core.sample.Sample at 0x7f5b79c22978>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Step6(data, samples, ipyclient, 123, True)\n",
    "s.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################] 100% 0:00:02 | concatenating inputs | s6 |\n"
     ]
    }
   ],
   "source": [
    "s.remote_build_concats_tier1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################] 100% 0:00:03 | clustering across 1  | s6 |\n"
     ]
    }
   ],
   "source": [
    "s.remote_cluster1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################] 100% 0:00:00 | concatenating inputs | s6 |\n"
     ]
    }
   ],
   "source": [
    "s.remote_build_concats_tier2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################] 100% 0:00:00 | clustering 2 | s6 |\n"
     ]
    }
   ],
   "source": [
    "s.remote_cluster2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#############       ]  66% 0:00:00 | building clusters    | s6 |\n"
     ]
    }
   ],
   "source": [
    "s.remote_build_denovo_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = \"4-refpairtest_across/4-refpairtest-tmpalign/4-refpairtest.chunk_105\"\n",
    "samples = s.samples\n",
    "data = s.data\n",
    "\n",
    "\n",
    "# data are already chunked, read in the whole thing\n",
    "with open(chunk, 'rt') as infile:\n",
    "    clusts = infile.read().split(\"//\\n//\\n\")[:-1]    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure sample name order\n",
    "samples.sort(key=lambda x: x.name)\n",
    "snames = [sample.name for sample in samples]\n",
    "\n",
    "# temp arrays for filtered clusters\n",
    "maxlen = 50\n",
    "filter_indels = np.zeros(len(clusts), dtype=np.bool_)\n",
    "filter_duples = np.zeros(len(clusts), dtype=np.bool_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a persistent shell for running muscle in. \n",
    "proc = sps.Popen(\n",
    "    [\"bash\"], \n",
    "    stdin=sps.PIPE, \n",
    "    stdout=sps.PIPE,\n",
    "    bufsize=0, \n",
    "    #universal_newlines=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iterate over clusters until finished\n",
    "allstack = []\n",
    "for ldx in range(len(clusts)):\n",
    "    # new alignment string for read1s and read2s\n",
    "    aligned = []\n",
    "    istack = []\n",
    "    lines = clusts[ldx].strip().split(\"\\n\")\n",
    "    names = lines[::2]\n",
    "    seqs = lines[1::2]        \n",
    "    align1 = []\n",
    "    align2 = []\n",
    "\n",
    "    # find duplicates and ...\n",
    "    if len(names) != len(set([x.rsplit(\"_\", 1)[0] for x in names])):\n",
    "        filter_duples[ldx] = True\n",
    "        istack = [consensus(seqs)]\n",
    "        \n",
    "    else:\n",
    "        names = [\">{};*{}\".format(j[1:], i) for i, j in enumerate(names)]\n",
    "        \n",
    "        try:\n",
    "            # try to split names on nnnn splitter\n",
    "            clust1, clust2 = zip(*[i.split(\"nnnn\") for i in seqs])\n",
    "\n",
    "            # make back into strings\n",
    "            cl1 = \"\\n\".join(chain(*zip(names, clust1)))\n",
    "            cl2 = \"\\n\".join(chain(*zip(names, clust2)))\n",
    "\n",
    "            # store allele (lowercase) info\n",
    "            shape = (len(seqs), max([len(i) for i in seqs]))\n",
    "            arrseqs = np.zeros(shape, dtype=\"S1\")\n",
    "            for row in range(arrseqs.shape[0]):\n",
    "                seqsrow = seqs[row]\n",
    "                arrseqs[row, :len(seqsrow)] = list(seqsrow)\n",
    "            amask = np.char.islower(arrseqs)\n",
    "            save_alleles = np.any(amask)\n",
    "            \n",
    "            print(save_alleles)\n",
    "        \n",
    "        finally:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus(seqs):\n",
    "    return seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TGCAGCTTCTGAAGCCTTCAATCCTCACGTCAACATGATGCCTACATGAATCATATACTGTTTATATTATCCTTATACACAAAAGAGCCTANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNTGTCCCTCACCTTCAGAGTACATGAGAGGTTCGACGCGTCGAATCATCTATMACAATGCGCGGCAGTTACGCCCCTCGGTATGAAAGCAAGACCCCCCCG']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "istack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    seeds = [\n",
    "        os.path.join(\n",
    "            data.dirs.across, \n",
    "            \"{}-{}.htemp\".format(data.name, jobid)) for jobid in jobids\n",
    "        ]\n",
    "    allseeds = os.path.join(data.dirs.across, \"{}-x.fa\".format(data.name))\n",
    "\n",
    "    cmd1 = ['cat'] + seeds\n",
    "    cmd2 = [ipyrad.bins.vsearch, '--sortbylength', '-', '--output', allseeds]\n",
    "    proc1 = sps.Popen(cmd1, stdout=sps.PIPE, close_fds=True)\n",
    "    proc2 = sps.Popen(cmd2, stdin=proc1.stdout, stdout=sps.PIPE, close_fds=True)\n",
    "    out = proc2.communicate()\n",
    "    proc1.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'', None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = s.data\n",
    "jobid = 2\n",
    "nthreads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # get files for this jobid\n",
    "    catshuf = os.path.join(\n",
    "        data.dirs.across, \n",
    "        \"{}-{}-catshuf.fa\".format(data.name, jobid))\n",
    "    uhaplos = os.path.join(\n",
    "        data.dirs.across, \n",
    "        \"{}-{}.utemp\".format(data.name, jobid))\n",
    "    hhaplos = os.path.join(\n",
    "        data.dirs.across, \n",
    "        \"{}-{}.htemp\".format(data.name, jobid))\n",
    "    #logfile = os.path.join(data.dirs.across, \"s6_cluster_stats.txt\")\n",
    "\n",
    "    ## parameters that vary by datatype\n",
    "    ## (too low of cov values yield too many poor alignments)\n",
    "    strand = \"plus\"\n",
    "    cov = 0.75         # 0.90\n",
    "    if data.paramsdict[\"datatype\"] in [\"gbs\", \"2brad\"]:\n",
    "        strand = \"both\"\n",
    "        cov = 0.60\n",
    "    elif data.paramsdict[\"datatype\"] == \"pairgbs\":\n",
    "        strand = \"both\"\n",
    "        cov = 0.75     # 0.90\n",
    "\n",
    "    ## nthreads is calculated in 'call_cluster()'\n",
    "    cmd = [ipyrad.bins.vsearch,\n",
    "           \"-cluster_smallmem\", catshuf,\n",
    "           \"-strand\", strand,\n",
    "           \"-query_cov\", str(cov),\n",
    "           \"-minsl\", str(0.5),\n",
    "           \"-id\", str(data.paramsdict[\"clust_threshold\"]),\n",
    "           \"-userout\", uhaplos,\n",
    "           \"-notmatched\", hhaplos,\n",
    "           \"-userfields\", \"query+target+qstrand\",\n",
    "           \"-maxaccepts\", \"1\",\n",
    "           \"-maxrejects\", \"0\",\n",
    "           \"-fasta_width\", \"0\",\n",
    "           \"-threads\", str(nthreads),  # \"0\",\n",
    "           \"-fulldp\",\n",
    "           \"-usersort\",\n",
    "           ]\n",
    "    proc = sps.Popen(cmd, stderr=sps.STDOUT, stdout=sps.PIPE)\n",
    "    out = proc.communicate()\n",
    "    if proc.returncode:\n",
    "        raise IPyradError(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = sps.Popen(cmd, stderr=sps.STDOUT, stdout=sps.PIPE)\n",
    "out = proc.communicate()\n",
    "if proc.returncode:\n",
    "    raise IPyradError(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_concat_files(s.data, 1, [s.data.samples[i] for i in s.cgroups[1]], 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = s.data\n",
    "jobid = 1\n",
    "samples = [s.data.samples[i] for i in s.cgroups[1]]\n",
    "randomseed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conshandles = [\n",
    "    sample.files.consens[0] for sample in samples if \n",
    "    sample.stats.reads_consens]\n",
    "conshandles.sort()\n",
    "assert conshandles, \"no consensus files found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    ## concatenate all of the gzipped consens files\n",
    "    cmd = ['cat'] + conshandles\n",
    "    groupcons = os.path.join(\n",
    "        data.dirs.across, \n",
    "        \"{}-{}-catcons.gz\".format(data.name, jobid))\n",
    "    LOGGER.debug(\" \".join(cmd))\n",
    "    with open(groupcons, 'w') as output:\n",
    "        call = sps.Popen(cmd, stdout=output, close_fds=True)\n",
    "        call.communicate()\n",
    "\n",
    "    ## a string of sed substitutions for temporarily replacing hetero sites\n",
    "    ## skips lines with '>', so it doesn't affect taxon names\n",
    "    subs = [\"/>/!s/W/A/g\", \"/>/!s/w/A/g\", \"/>/!s/R/A/g\", \"/>/!s/r/A/g\",\n",
    "            \"/>/!s/M/A/g\", \"/>/!s/m/A/g\", \"/>/!s/K/T/g\", \"/>/!s/k/T/g\",\n",
    "            \"/>/!s/S/C/g\", \"/>/!s/s/C/g\", \"/>/!s/Y/C/g\", \"/>/!s/y/C/g\"]\n",
    "    subs = \";\".join(subs)\n",
    "\n",
    "    ## impute pseudo-haplo information to avoid mismatch at hetero sites\n",
    "    ## the read data with hetero sites is put back into clustered data later.\n",
    "    ## pipe passed data from gunzip to sed.\n",
    "    cmd1 = [\"gunzip\", \"-c\", groupcons]\n",
    "    cmd2 = [\"sed\", subs]\n",
    "    LOGGER.debug(\" \".join(cmd1))\n",
    "    LOGGER.debug(\" \".join(cmd2))\n",
    "\n",
    "    proc1 = sps.Popen(cmd1, stdout=sps.PIPE, close_fds=True)\n",
    "    allhaps = groupcons.replace(\"-catcons.gz\", \"-cathaps.fa\")\n",
    "    with open(allhaps, 'w') as output:\n",
    "        proc2 = sps.Popen(cmd2, stdin=proc1.stdout, stdout=output, close_fds=True)\n",
    "        proc2.communicate()\n",
    "    proc1.stdout.close()\n",
    "\n",
    "    ## now sort the file using vsearch\n",
    "    allsort = groupcons.replace(\"-catcons.gz\", \"-catsort.fa\")\n",
    "    cmd1 = [ipyrad.bins.vsearch,\n",
    "            \"--sortbylength\", allhaps,\n",
    "            \"--fasta_width\", \"0\",\n",
    "            \"--output\", allsort]\n",
    "    LOGGER.debug(\" \".join(cmd1))\n",
    "    proc1 = sps.Popen(cmd1, close_fds=True)\n",
    "    proc1.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dirs.across"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the theading of cluster1 jobs:\n",
    "ncpus = len(self.ipyclient.ids)\n",
    "njobs = len(self.cgroups)\n",
    "nnodes = len(self.hostd)\n",
    "minthreads = 2\n",
    "maxthreads = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product\n",
    "targets = [0, 1]\n",
    "self.thview = self.ipyclient.load_balanced_view(targets=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################] 100% 0:00:02 | concatenating inputs | s6 |\n"
     ]
    }
   ],
   "source": [
    "s.prepare_concats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IPyradError",
     "evalue": "(b'vsearch v2.8.0_linux_x86_64, 15.5GB RAM, 4 cores\\nhttps://github.com/torognes/vsearch\\n\\n\\n\\nUnable to open file for reading (/home/deren/Documents/ipyrad/tests/4-refpairtest_across/4-refpairtest-2-catshuf.fa)\\n', None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIPyradError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-38c961b35fd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcluster1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-0187c2dc7daf>\u001b[0m in \u001b[0;36mcluster1\u001b[0;34m(data, jobid, nthreads)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIPyradError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIPyradError\u001b[0m: (b'vsearch v2.8.0_linux_x86_64, 15.5GB RAM, 4 cores\\nhttps://github.com/torognes/vsearch\\n\\n\\n\\nUnable to open file for reading (/home/deren/Documents/ipyrad/tests/4-refpairtest_across/4-refpairtest-2-catshuf.fa)\\n', None)"
     ]
    }
   ],
   "source": [
    "cluster1(data, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cluster1(data, jobid, nthreads):\n",
    "\n",
    "    # get files for this jobid\n",
    "    catshuf = os.path.join(\n",
    "        data.dirs.across, \n",
    "        \"{}-{}-catshuf.fa\".format(data.name, jobid))\n",
    "    uhaplos = os.path.join(\n",
    "        data.dirs.across, \n",
    "        \"{}-{}.utemp\".format(data.name, jobid))\n",
    "    hhaplos = os.path.join(\n",
    "        data.dirs.across, \n",
    "        \"{}-{}.htemp\".format(data.name, jobid))\n",
    "\n",
    "    ## parameters that vary by datatype\n",
    "    ## (too low of cov values yield too many poor alignments)\n",
    "    strand = \"plus\"\n",
    "    cov = 0.75         # 0.90\n",
    "    if data.paramsdict[\"datatype\"] in [\"gbs\", \"2brad\"]:\n",
    "        strand = \"both\"\n",
    "        cov = 0.60\n",
    "    elif data.paramsdict[\"datatype\"] == \"pairgbs\":\n",
    "        strand = \"both\"\n",
    "        cov = 0.75     # 0.90\n",
    "\n",
    "    ## nthreads is calculated in 'call_cluster()'\n",
    "    cmd = [ipyrad.bins.vsearch,\n",
    "           \"-cluster_smallmem\", catshuf,\n",
    "           \"-strand\", strand,\n",
    "           \"-query_cov\", str(cov),\n",
    "           \"-minsl\", str(0.5),\n",
    "           \"-id\", str(data.paramsdict[\"clust_threshold\"]),\n",
    "           \"-userout\", uhaplos,\n",
    "           \"-notmatched\", hhaplos,\n",
    "           \"-userfields\", \"query+target+qstrand\",\n",
    "           \"-maxaccepts\", \"1\",\n",
    "           \"-maxrejects\", \"0\",\n",
    "           \"-fasta_width\", \"0\",\n",
    "           \"-threads\", str(nthreads),  # \"0\",\n",
    "           \"-fulldp\",\n",
    "           \"-usersort\",\n",
    "           ]\n",
    "    proc = sps.Popen(cmd, stderr=sps.STDOUT, stdout=sps.PIPE)\n",
    "    out = proc.communicate()\n",
    "    if proc.returncode:\n",
    "        raise IPyradError(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(self.samples) < 50:\n",
    "    nclusters = 1\n",
    "elif len(self.samples) < 100:\n",
    "    nclusters = 2\n",
    "elif len(self.samples) < 200:\n",
    "    nclusters = 4\n",
    "elif len(self.samples) > 500:\n",
    "    nclusters = 10\n",
    "else:\n",
    "    nclusters = int(len(self.samples) / 10)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['1A_0', '1B_0', '1C_0', '1D_0'],\n",
       " 1: ['2E_0', '2F_0', '2G_0', '2H_0'],\n",
       " 2: ['3I_0', '3J_0', '3K_0', '3L_0']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.cgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'oud': [0, 1, 2, 3]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hosts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
