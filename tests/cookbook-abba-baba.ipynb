{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cookbook: abba-baba introgression stats\n",
    "\n",
    "The *ipyrad.analysis* Python module includes functions to calculate abba-baba admixture statistics, as well as several variants of these measures, and to perform signifance tests. The code in this notebook is all Python, which you can copy/paste into an IPython terminal to execute, or, preferably, run in a Jupyter notebook like this one. See the other analysis cookbooks for [instructions](http://ipyrad.readthedocs.io/analysis.html) on using Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## start by loading several Python librarires\n",
    "import ipyrad as ip\n",
    "import ipyrad.analysis as ipa\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loading Assembly: cli\n",
      "  from saved path: ~/Documents/ipyrad/tests/cli/cli.json\n"
     ]
    }
   ],
   "source": [
    "data = ip.load_json(\"cli/cli.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geno : /home/deren/Documents/ipyrad/tests/cli/cli_outfiles/cli.geno\n",
       "loci : /home/deren/Documents/ipyrad/tests/cli/cli_outfiles/cli.loci\n",
       "nexus : /home/deren/Documents/ipyrad/tests/cli/cli_outfiles/cli.nex\n",
       "phy : /home/deren/Documents/ipyrad/tests/cli/cli_outfiles/cli.phy\n",
       "snpsmap : /home/deren/Documents/ipyrad/tests/cli/cli_outfiles/cli.snps.map\n",
       "snpsphy : /home/deren/Documents/ipyrad/tests/cli/cli_outfiles/cli.snps.phy\n",
       "vcf : /home/deren/Documents/ipyrad/tests/cli/cli_outfiles/cli.vcf\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.outfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = [[\"1A_0\"], \n",
    "        [\"1C_0\"], \n",
    "        [\"2E_0\", \"2F_0\"],\n",
    "        [\"3I_0\", \"3J_0\"]]\n",
    "handle = data.outfiles.loci\n",
    "maxlen = 200\n",
    "ntips = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make loci parsing script\n",
    "This takes a .loci file and converts it to an array of the format:\n",
    "\n",
    "(4 or 5, n-variable-loci, maxsnps)\n",
    "e.g., (4, 1000, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _loci2loci(handle, taxonlist, maxlen=200):\n",
    "    \"\"\"\n",
    "    sssss\n",
    "    \"\"\"\n",
    "\n",
    "    ## read in the input file\n",
    "    with open(handle, 'r') as infile:\n",
    "        loci = infile.read().strip().split(\"|\\n\")\n",
    "        nloci = len(loci)\n",
    "\n",
    "    ## count max snps \n",
    "    maxsnps = 0\n",
    "    for iloc in xrange(nloci):\n",
    "        lines = loci[iloc].split(\"//\")[1].strip()\n",
    "        _maxl = len([i for i in lines.split(\"|\")[0].strip() if i != \" \"])\n",
    "        maxsnps = max(maxsnps, _maxl)\n",
    "\n",
    "    ## build array ({4or5}, nloci, maxsnps)\n",
    "    arr = np.zeros((nloci, 4, maxsnps), dtype=np.float)\n",
    "       \n",
    "    ## get the outgroup allele (most common in outgroup)\n",
    "    for loc in xrange(10): #nloci):\n",
    "        ## parse the locus\n",
    "        lines = loci[loc].split(\"\\n\")[:-1]\n",
    "        names = [i.split()[0] for i in lines]\n",
    "        seqs = np.array([list(i.split()[1]) for i in lines])\n",
    "        snpidx = lines[-1]\n",
    "        \n",
    "        ## find most frequent allele among outgroups and call that the\n",
    "        ## outgroup allele (A). How do we break ties? For simplicity, we'll\n",
    "        ## consistently choose lowest base to break ties (e.g., A over C)\n",
    "        arr[:, -1].fill(1)\n",
    "        \n",
    "        ## \n",
    "        #for otax in taxonlist[-1]:\n",
    "        #    print seqs[names.index(otax)]\n",
    "        \n",
    "        #print(seqs)\n",
    "        #print(names)\n",
    "        #seqlen = seqs.shape[1]\n",
    "\n",
    "        #taxi = sum([i in names for i in taxonlist])\n",
    "        #taxc[iloc] += taxi        \n",
    "        #print(loci[loc])\n",
    "\n",
    "    return arr\n",
    "    \n",
    "    \n",
    "#print maxlen\n",
    "arr = _loci2loci(handle, test)\n",
    "arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "            arr = np.zeros((5, maxlen), dtype=np.float64)\n",
    "            ## find most frequent allele among outgroups and call that the\n",
    "            ## outgroup allele (A). How do we break ties? For simplicity, we'll\n",
    "            ## consistently choose lowest base to break ties (e.g., A over C)\n",
    "            arr[-1].fill(1)\n",
    "\n",
    "            ## fill fake data columns with 9s\n",
    "            arr[:, seqlen-maxlen:].fill(9)\n",
    "\n",
    "            ## get outgroup values\n",
    "            outvals = seqs[names.index(taxonlist[4])]\n",
    "            for itax in xrange(4):\n",
    "                ## make 1s all sites that match to outg\n",
    "                tmparr = np.int8(seqs[names.index(taxonlist[itax])] == outvals)\n",
    "                ## make 9s all sites that have (N-RKSMW)\n",
    "                #tmparr[\n",
    "                arr[itax][:tmparr.shape[0]] = tmparr\n",
    "            farr[iloc] = arr\n",
    "\n",
    "    ## warn if no SNPs are found\n",
    "    ## warn if no loci have sampling of all taxa\n",
    "\n",
    "    #print(np.histogram(taxc, range(7)))\n",
    "    ## return array that includes np.ones for loci w/o taxa\n",
    "    return farr[taxc == len(taxonlist)], taxc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'taxonlist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-298de83708f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mseqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtaxonlist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtaxc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtaxi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtaxi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxonlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'taxonlist' is not defined"
     ]
    }
   ],
   "source": [
    "with open(handle, 'r') as infile:\n",
    "    ## split on \"//\" for legacy compatibility\n",
    "    loci = infile.read().strip().split(\"|\\n\")\n",
    "\n",
    "## create emtpy array to fill\n",
    "nloci = len(loci)\n",
    "farr = np.ones((nloci, ntips, maxlen), dtype=np.float64)\n",
    "taxc = np.zeros((nloci,))\n",
    "\n",
    "## iterate over loci to find those which have taxon sampling\n",
    "for iloc in xrange(nloci):\n",
    "    lines = loci[iloc].split(\"//\")[0].split()\n",
    "    names = [i[1:] for i in lines[::2]]\n",
    "    seqs = np.array([list(i) for i in lines[1::2]])\n",
    "\n",
    "    taxi = sum([i in names for i in taxonlist])\n",
    "    taxc[iloc] += taxi\n",
    "    if taxi == len(taxonlist):\n",
    "        arr = np.zeros((5, maxlen), dtype=np.float64)\n",
    "print names\n",
    "print seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['T' 'T' 'A' 'G' 'T' 'T' 'C' 'T' 'T' 'A' 'G' 'A' 'C' 'T' 'A' 'T' 'T' 'C'\n",
      "  'G' 'T' 'T' 'A' 'A' 'C' 'T' 'C' 'G' 'A' 'G' 'G' 'C' 'G' 'A' 'G' 'T' 'G'\n",
      "  'C' 'C' 'C' 'T' 'A' 'A' 'G' 'C' 'G' 'C' 'T' 'A' 'T' 'A' 'C' 'G' 'T' 'G'\n",
      "  'G' 'C' 'A' 'G' 'G' 'A' 'C' 'C' 'T' 'G' 'T' 'T' 'G' 'G' 'A' 'A' 'A' 'A'\n",
      "  'A' 'C' 'A' 'C' 'G' 'C' 'A' 'G' 'A']\n",
      " ['T' 'T' 'A' 'G' 'T' 'T' 'C' 'T' 'T' 'A' 'G' 'A' 'C' 'T' 'A' 'T' 'T' 'C'\n",
      "  'G' 'T' 'T' 'A' 'A' 'C' 'T' 'C' 'G' 'A' 'G' 'G' 'C' 'G' 'A' 'G' 'T' 'G'\n",
      "  'C' 'C' 'C' 'T' 'A' 'A' 'G' 'C' 'G' 'C' 'T' 'A' 'T' 'A' 'C' 'G' 'T' 'G'\n",
      "  'G' 'C' 'A' 'G' 'G' 'A' 'C' 'C' 'T' 'G' 'T' 'T' 'G' 'G' 'A' 'A' 'A' 'A'\n",
      "  'A' 'C' 'A' 'C' 'G' 'C' 'A' 'G' 'A']\n",
      " ['T' 'T' 'A' 'G' 'T' 'T' 'C' 'T' 'T' 'A' 'G' 'A' 'C' 'T' 'A' 'T' 'T' 'C'\n",
      "  'G' 'T' 'T' 'A' 'A' 'C' 'T' 'C' 'G' 'A' 'G' 'G' 'C' 'G' 'A' 'G' 'T' 'G'\n",
      "  'C' 'C' 'C' 'T' 'A' 'A' 'G' 'C' 'G' 'C' 'T' 'A' 'T' 'A' 'C' 'G' 'T' 'G'\n",
      "  'G' 'C' 'A' 'G' 'G' 'A' 'C' 'C' 'T' 'G' 'T' 'T' 'G' 'G' 'A' 'A' 'A' 'A'\n",
      "  'A' 'C' 'A' 'C' 'G' 'C' 'A' 'G' 'A']\n",
      " ['T' 'T' 'A' 'G' 'T' 'T' 'C' 'T' 'T' 'A' 'G' 'A' 'C' 'T' 'A' 'T' 'T' 'C'\n",
      "  'G' 'T' 'T' 'A' 'A' 'C' 'T' 'C' 'G' 'A' 'G' 'G' 'C' 'G' 'A' 'G' 'T' 'G'\n",
      "  'C' 'C' 'C' 'T' 'A' 'A' 'G' 'C' 'G' 'C' 'T' 'A' 'C' 'A' 'C' 'G' 'T' 'G'\n",
      "  'G' 'C' 'A' 'G' 'G' 'A' 'C' 'C' 'T' 'G' 'T' 'T' 'G' 'G' 'A' 'A' 'A' 'A'\n",
      "  'A' 'C' 'A' 'C' 'G' 'C' 'A' 'G' 'A']\n",
      " ['T' 'T' 'A' 'G' 'T' 'T' 'C' 'T' 'T' 'A' 'G' 'A' 'C' 'T' 'A' 'T' 'T' 'C'\n",
      "  'G' 'T' 'T' 'A' 'A' 'C' 'T' 'C' 'G' 'A' 'G' 'G' 'C' 'G' 'A' 'G' 'T' 'G'\n",
      "  'C' 'C' 'C' 'T' 'A' 'A' 'G' 'C' 'G' 'C' 'T' 'A' 'C' 'A' 'C' 'G' 'T' 'G'\n",
      "  'G' 'C' 'A' 'G' 'G' 'A' 'C' 'C' 'T' 'G' 'T' 'T' 'G' 'G' 'A' 'A' 'A' 'A'\n",
      "  'A' 'C' 'A' 'C' 'G' 'C' 'A' 'G' 'A']\n",
      " ['T' 'T' 'A' 'G' 'T' 'T' 'C' 'T' 'T' 'A' 'G' 'A' 'C' 'T' 'A' 'T' 'T' 'C'\n",
      "  'G' 'T' 'T' 'A' 'A' 'C' 'T' 'C' 'G' 'A' 'G' 'G' 'C' 'G' 'A' 'G' 'T' 'G'\n",
      "  'C' 'C' 'C' 'T' 'A' 'A' 'G' 'C' 'G' 'C' 'T' 'A' 'T' 'A' 'C' 'G' 'T' 'G'\n",
      "  'G' 'C' 'A' 'G' 'G' 'A' 'C' 'C' 'T' 'G' 'T' 'T' 'G' 'G' 'A' 'A' 'A' 'A'\n",
      "  'A' 'C' 'A' 'C' 'G' 'C' 'A' 'G' 'A']\n",
      " ['T' 'T' 'A' 'G' 'T' 'T' 'C' 'T' 'T' 'A' 'G' 'A' 'C' 'T' 'A' 'T' 'T' 'C'\n",
      "  'G' 'T' 'T' 'A' 'A' 'C' 'T' 'C' 'G' 'A' 'G' 'G' 'C' 'G' 'A' 'G' 'T' 'G'\n",
      "  'C' 'C' 'C' 'T' 'A' 'A' 'G' 'C' 'G' 'C' 'T' 'A' 'C' 'A' 'C' 'G' 'T' 'G'\n",
      "  'G' 'C' 'A' 'G' 'G' 'A' 'C' 'C' 'T' 'G' 'T' 'T' 'G' 'G' 'A' 'A' 'A' 'A'\n",
      "  'A' 'C' 'A' 'C' 'G' 'C' 'A' 'G' 'A']\n",
      " ['T' 'T' 'A' 'G' 'T' 'T' 'C' 'T' 'T' 'A' 'G' 'A' 'C' 'T' 'A' 'T' 'T' 'C'\n",
      "  'G' 'T' 'T' 'A' 'A' 'C' 'T' 'C' 'G' 'A' 'G' 'G' 'C' 'G' 'A' 'G' 'T' 'G'\n",
      "  'C' 'C' 'C' 'T' 'A' 'A' 'G' 'C' 'G' 'C' 'T' 'A' 'T' 'A' 'C' 'G' 'T' 'G'\n",
      "  'G' 'C' 'A' 'G' 'G' 'A' 'C' 'C' 'T' 'G' 'T' 'T' 'G' 'G' 'A' 'A' 'A' 'A'\n",
      "  'A' 'C' 'A' 'C' 'G' 'C' 'A' 'G' 'A']\n",
      " ['T' 'T' 'A' 'G' 'T' 'T' 'C' 'T' 'T' 'A' 'G' 'A' 'C' 'T' 'A' 'T' 'T' 'C'\n",
      "  'G' 'T' 'T' 'A' 'A' 'C' 'T' 'C' 'G' 'A' 'G' 'G' 'C' 'G' 'A' 'G' 'T' 'G'\n",
      "  'C' 'C' 'C' 'T' 'A' 'A' 'G' 'C' 'G' 'C' 'T' 'A' 'T' 'A' 'C' 'G' 'T' 'G'\n",
      "  'G' 'C' 'A' 'G' 'G' 'A' 'C' 'C' 'T' 'G' 'T' 'T' 'G' 'G' 'A' 'A' 'A' 'A'\n",
      "  'A' 'C' 'A' 'C' 'G' 'C' 'A' 'G' 'A']\n",
      " ['T' 'T' 'A' 'G' 'T' 'T' 'C' 'T' 'T' 'A' 'G' 'A' 'C' 'T' 'A' 'T' 'T' 'C'\n",
      "  'G' 'T' 'T' 'A' 'A' 'C' 'T' 'C' 'G' 'A' 'G' 'G' 'C' 'G' 'A' 'G' 'T' 'G'\n",
      "  'C' 'C' 'C' 'T' 'A' 'A' 'G' 'C' 'G' 'C' 'T' 'A' 'T' 'A' 'C' 'G' 'T' 'G'\n",
      "  'G' 'C' 'A' 'G' 'G' 'A' 'C' 'C' 'T' 'G' 'T' 'T' 'G' 'G' 'A' 'A' 'A' 'A'\n",
      "  'A' 'C' 'A' 'C' 'G' 'C' 'A' 'G' 'A']\n",
      " ['T' 'T' 'A' 'G' 'T' 'T' 'C' 'T' 'T' 'A' 'G' 'A' 'C' 'T' 'A' 'T' 'T' 'C'\n",
      "  'G' 'T' 'T' 'A' 'A' 'C' 'T' 'C' 'G' 'A' 'G' 'G' 'C' 'G' 'A' 'G' 'T' 'G'\n",
      "  'C' 'C' 'C' 'T' 'A' 'A' 'G' 'C' 'G' 'C' 'T' 'A' 'A' 'A' 'C' 'G' 'T' 'G'\n",
      "  'G' 'C' 'A' 'G' 'G' 'A' 'C' 'C' 'T' 'G' 'T' 'T' 'G' 'G' 'A' 'A' 'A' 'A'\n",
      "  'A' 'C' 'A' 'C' 'G' 'C' 'A' 'G' 'A']]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'taxonlist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-87cb626db8bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mseqlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtaxonlist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtaxc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtaxi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtaxi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxonlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'taxonlist' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "    print seqs\n",
    "    seqlen = seqs.shape[1]\n",
    "\n",
    "    taxi = sum([i in names for i in taxonlist])\n",
    "    taxc[iloc] += taxi\n",
    "    if taxi == len(taxonlist):\n",
    "        arr = np.zeros((5, maxlen), dtype=np.float64)\n",
    "        ## find most frequent allele among outgroups and call that the\n",
    "        ## outgroup allele (A). How do we break ties? For simplicity, we'll\n",
    "        ## consistently choose lowest base to break ties (e.g., A over C)\n",
    "        arr[-1].fill(1)\n",
    "\n",
    "        ## fill fake data columns with 9s\n",
    "        arr[:, seqlen-maxlen:].fill(9)\n",
    "\n",
    "        ## get outgroup values\n",
    "        outvals = seqs[names.index(taxonlist[4])]\n",
    "        for itax in xrange(4):\n",
    "            ## make 1s all sites that match to outg\n",
    "            tmparr = np.int8(seqs[names.index(taxonlist[itax])] == outvals)\n",
    "            ## make 9s all sites that have (N-RKSMW)\n",
    "            #tmparr[\n",
    "            arr[itax][:tmparr.shape[0]] = tmparr\n",
    "        farr[iloc] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4, 200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "farr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b4a312e106bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## parse the loci file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mseqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mseqlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lines' is not defined"
     ]
    }
   ],
   "source": [
    "iloc = 0\n",
    "\n",
    "## parse the loci file\n",
    "names = lines[::2]\n",
    "seqs = np.array([list(i) for i in lines[1::2]])\n",
    "seqlen = seqs.shape[1]\n",
    "print seqlen\n",
    "\n",
    "## check for taxon coverage\n",
    "taxi = [any([i in names for i in ptax]) for ptax in test]\n",
    "print taxi\n",
    "\n",
    "## group sequences into arr[arrs] \n",
    "sidx = [[names.index(i) for i in ptax] for ptax in test]\n",
    "seqs = np.array([np.array([seqs[i] for i in isid]).view(np.int8) for isid in sidx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'taxi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f909a1a69aa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## choose test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mntips\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mp3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtaxi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mntips\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mp3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtaxi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaxi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'taxi' is not defined"
     ]
    }
   ],
   "source": [
    "## choose test\n",
    "if ntips == 4:\n",
    "    p3 = [taxi[2]]\n",
    "elif ntips == 5:\n",
    "    p3 = [taxi[2], taxi[3]]\n",
    "else:\n",
    "    raise IPyradWarningExit(\"ntips can only be 4 or 5\")   \n",
    "print ntips\n",
    "\n",
    "\n",
    "## filter for loci w/ sufficient coverage\n",
    "suff = all([taxi[0], taxi[1], taxi[-1]]) & any(p3)\n",
    "if suff:\n",
    "    ## an array to fill\n",
    "    arr = np.zeros((ntips, maxlen), dtype=np.float64)\n",
    "    print arr.shape\n",
    "    \n",
    "    ## fill outgroup with most common base in outgroup\n",
    "    ancestral = reftrick(seqs[-1], GETCONS)[:, 0]\n",
    "    for i in xrange(ntips):\n",
    "        arr[i][:seqlen] = np.sum(seqs[i] == ancestral, axis=0) / \\\n",
    "                                float(seqs[i].shape[0])\n",
    "    print ancestral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['R' 'G' 'A']\n",
      " ['K' 'G' 'T']\n",
      " ['S' 'G' 'C']\n",
      " ['Y' 'T' 'C']\n",
      " ['W' 'T' 'A']\n",
      " ['M' 'C' 'A']]\n",
      "[[82 71 65]\n",
      " [75 71 84]\n",
      " [83 71 67]\n",
      " [89 84 67]\n",
      " [87 84 65]\n",
      " [77 67 65]]\n",
      "(2, 81)\n",
      "['T' 'T' 'A' 'G' 'T' 'T' 'C' 'T' 'T' 'A' 'G' 'A' 'C' 'T' 'A' 'T' 'T' 'C'\n",
      " 'G' 'T' 'T' 'A' 'A' 'C' 'T' 'C' 'G' 'A' 'G' 'G' 'C' 'G' 'A' 'G' 'T' 'G'\n",
      " 'C' 'C' 'C' 'T' 'A' 'A' 'G' 'C' 'G' 'C' 'T' 'A' 'T' 'A' 'C' 'G' 'T' 'G'\n",
      " 'G' 'C' 'A' 'G' 'G' 'A' 'C' 'C' 'T' 'G' 'T' 'T' 'G' 'G' 'A' 'A' 'A' 'A'\n",
      " 'A' 'C' 'A' 'C' 'G' 'C' 'A' 'G' 'A']\n",
      "84\n",
      "[[ 84.  84.  65.  71.  84.  84.  67.  84.  84.  65.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   0.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.]\n",
      " [ 84.  84.  65.  71.  84.  84.  67.  84.  84.  65.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   0.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.]\n",
      " [ 84.  84.  65.  71.  84.  84.  67.  84.  84.  65.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   0.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.]\n",
      " [ 84.  84.  65.  71.  84.  84.  67.  84.  84.  65.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   0.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "    1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.]]\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "print GETCONS.view(\"S1\")\n",
    "print GETCONS\n",
    "print iseq.shape\n",
    "print ancestral.view(\"S1\")\n",
    "\n",
    "print iseq[0][0]\n",
    "\n",
    "\n",
    "for seq in xrange(iseq.shape[0]):\n",
    "    for col in xrange(10):#iseq.shape[1]):\n",
    "        ## expand colums with ambigs and remove N-\n",
    "        base = iseq[seq][col]\n",
    "        if base in consdict[:, 0]:\n",
    "            who = np.where(consdict[:, 0] == base)[0]\n",
    "            amseq[(seq*2)][col] = consdict[who][0]\n",
    "            amseq[(seq*2)+1][col] = consdict[who][1]\n",
    "        else:\n",
    "            amseq[(seq*2)][col] = base\n",
    "            amseq[(seq*2)+1][col] = base \n",
    "print amseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ancestral' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e73a77452dab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrefseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mancestral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0miseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconsdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGETCONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ancestral' is not defined"
     ]
    }
   ],
   "source": [
    "refseq = ancestral\n",
    "iseq = seqs[2]\n",
    "consdict = GETCONS\n",
    "\n",
    "@numba.jit()\n",
    "def reffreq(refseq, iseq, consdict):\n",
    "    ## empty arrays\n",
    "    freq = np.zeros((iseq.shape[1]), dtype=np.float64)\n",
    "    amseq = np.zeros((iseq.shape[0]*2, iseq.shape[1]), dtype=np.int8)\n",
    "\n",
    "    ## fill in both copies\n",
    "    for seq in xrange(iseq.shape[0]):\n",
    "        for col in xrange(iseq.shape[1]):\n",
    "            ## expand colums with ambigs and remove N-\n",
    "            base = iseq[seq][col]\n",
    "            if base in consdict[:, 0]:\n",
    "                who = np.where(consdict[:, 0] == base)[0]\n",
    "                amseq[(seq*2)][col] = consdict[who][0]\n",
    "                amseq[(seq*2)+1][col] = consdict[who][1]\n",
    "            else:\n",
    "                amseq[(seq*2)][col] = base\n",
    "                amseq[(seq*2)+1][col] = base\n",
    "\n",
    "    ## get as frequencies\n",
    "    amseq = (refseq == amseq).astype(np.float64)\n",
    "    for i in xrange(amseq.shape[0]):\n",
    "        freq += amseq[i]\n",
    "    return freq / np.float64(amseq.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[82, 71, 65],\n",
       "       [75, 71, 84],\n",
       "       [83, 71, 67],\n",
       "       [89, 84, 67],\n",
       "       [87, 84, 65],\n",
       "       [77, 67, 65]], dtype=uint8)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-9a3725ca3f32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m#    amseq[np.int8((seq*2)+1)][col] = base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreffreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGETCONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def reffreq(refseq, iseq, consdict):\n",
    "    ## empty arrays\n",
    "    freq = np.zeros((iseq.shape[1]), dtype=np.float64)\n",
    "    amseq = np.zeros((iseq.shape[0]*2, iseq.shape[1]), dtype=np.uint8)\n",
    "    \n",
    "    ## fill in both copies\n",
    "    for seq in xrange(iseq.shape[0]):\n",
    "        for col in xrange(iseq.shape[1]):\n",
    "            ## expand colums with ambigs and remove N-\n",
    "            base = np.uint8(iseq[seq][col])\n",
    "            who = np.where(base == consdict[:, 0])[0]\n",
    "            #if np.any(who):\n",
    "            return amseq[np.uint8(seq*2)][np.uint8(col)] #= consdict[who][0]\n",
    "            #    amseq[np.int8((seq*2)+1)][col] = consdict[who][1]\n",
    "            #else:\n",
    "            #    amseq[np.int8(seq*2)][col] = base\n",
    "            #    amseq[np.int8((seq*2)+1)][col] = base\n",
    "\n",
    "np.dtype(reffreq(refseq, seqs[1], GETCONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%timeit \n",
    "#arr = seqs.view(np.int8)\n",
    "from ipyrad.assemble.write_outfiles import *\n",
    "#outg = reftrick(seqs[-1], GETCONS).view(\"S1\")\n",
    "#print outg[:, 0]\n",
    "#print seqs[-1].view(\"S1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "## iterate over loci to find those which have taxon sampling\n",
    "for iloc in xrange(nloci):\n",
    "    \n",
    "    ## parse the loci file\n",
    "    lines = loci[iloc].split(\"//\")[0].split()\n",
    "    names = lines[::2]\n",
    "    seqs = np.array([list(i) for i in lines[1::2]])\n",
    "    seqlen = seqs.shape[1]\n",
    "\n",
    "    ## check for taxon coverage\n",
    "    taxi = [any([i in names for i in ptax]) for ptax in test]\n",
    "    \n",
    "    ## group sequences into arr[arrs] \n",
    "    sidx = [[names.index(i) for i in ptax] for ptax in test]\n",
    "    seqs = np.array([np.array([seqs[i] for i in isid]).view(np.int8) for isid in sidx])\n",
    "    \n",
    "    ## choose test\n",
    "    if ntips == 4:\n",
    "        p3 = [taxi[2]]\n",
    "    elif ntips == 5:\n",
    "        p3 = [taxi[2], taxi[3]]\n",
    "    else:\n",
    "        raise IPyradWarningExit(\"ntips can only be 4 or 5\")\n",
    "        \n",
    "    ## filter for loci w/ sufficient coverage\n",
    "    if all([taxi[0], taxi[1], taxi[-1]]) & any(p3):\n",
    "        \n",
    "        ## an array to fill\n",
    "        arr = np.zeros((seqs.shape[0], maxlen), dtype=np.float64)\n",
    "        \n",
    "        ## fill outgroup with most common base in outgroup\n",
    "        ancestral = reftrick(seqs[-1], GETCONS)[:, 0]\n",
    "        for i in xrange(seqs.shape[0]):\n",
    "            arr[i][:seqlen] = np.sum(seqs[i] == ancestral, axis=0) / \\\n",
    "                                 float(seqs[i].shape[0])\n",
    "\n",
    "print arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2,81) into shape (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-2e11ef46ae87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtmparr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxonlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitax\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moutvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m## make 9s all sites that have (N-RKSMW)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitax\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtmparr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmparr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mfarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2,81) into shape (2)"
     ]
    }
   ],
   "source": [
    "        \n",
    "        arr[-1].fill(1)\n",
    "        ## fill fake data columns with 9s\n",
    "        arr[:, seqlen-maxlen:].fill(9)\n",
    "\n",
    "        ## get outgroup values\n",
    "        outvals = seqs[names.index(taxonlist[3])]\n",
    "        for itax in xrange(4):\n",
    "            ## make 1s all sites that match to outg\n",
    "            tmparr = np.int8(seqs[names.index(taxonlist[itax])] == outvals)\n",
    "            ## make 9s all sites that have (N-RKSMW)\n",
    "            arr[itax][:tmparr.shape[0]] = tmparr\n",
    "        farr[iloc] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9]],\n",
       "\n",
       "       [[1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9]],\n",
       "\n",
       "       [[1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9]],\n",
       "\n",
       "       ..., \n",
       "       [[1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9]],\n",
       "\n",
       "       [[0, 1, 1, ..., 9, 9, 9],\n",
       "        [0, 1, 1, ..., 9, 9, 9],\n",
       "        [0, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9]],\n",
       "\n",
       "       [[1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9],\n",
       "        [1, 1, 1, ..., 9, 9, 9]]], dtype=int8)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "farr[taxc == len(taxonlist)]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
