{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipyrad as ip\n",
    "from collections import Counter\n",
    "\n",
    "import scipy.stats\n",
    "import scipy.misc\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arrayed = np.array([list(\"AAAATnnnnGGGG\"), list(\"AAAA-nnnnGGGG\"), \n",
    "                    list(\"AAAA-nnnnGGGG\"), list(\"AAAA-nnnnGGGG\"), \n",
    "                    list(\"AAAA-nnnnGGGG\"), list(\"AAAA-nnnnGGGG\"), \n",
    "                    list(\"AAAA-nnnnGGGG\")])\n",
    "\n",
    "consens = list(\"AAAANnnnnGGGG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A' 'A' 'A' 'A' 'T' 'n' 'n' 'n' 'n' 'G' 'G' 'G' 'G']\n",
      " ['A' 'A' 'A' 'A' '-' 'n' 'n' 'n' 'n' 'G' 'G' 'G' 'G']\n",
      " ['A' 'A' 'A' 'A' '-' 'n' 'n' 'n' 'n' 'G' 'G' 'G' 'G']\n",
      " ['A' 'A' 'A' 'A' '-' 'n' 'n' 'n' 'n' 'G' 'G' 'G' 'G']\n",
      " ['A' 'A' 'A' 'A' '-' 'n' 'n' 'n' 'n' 'G' 'G' 'G' 'G']\n",
      " ['A' 'A' 'A' 'A' '-' 'n' 'n' 'n' 'n' 'G' 'G' 'G' 'G']\n",
      " ['A' 'A' 'A' 'A' '-' 'n' 'n' 'n' 'n' 'G' 'G' 'G' 'G']]\n",
      "['A', 'A', 'A', 'A', 'N', 'n', 'n', 'n', 'n', 'G', 'G', 'G', 'G']\n"
     ]
    }
   ],
   "source": [
    "print arrayed\n",
    "print consens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loading Assembly: cli\n",
      "  from saved path: ~/Documents/ipyrad/tests/cli/cli.json\n"
     ]
    }
   ],
   "source": [
    "data = ip.load_json(\"cli/cli.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basecall(site, data):\n",
    "    \"\"\" prepares stack for making base calls \"\"\"\n",
    "    ## count em\n",
    "    site = Counter(site)\n",
    "\n",
    "    ## remove Ns and (-)s\n",
    "    if \"N\" in site:\n",
    "        site.pop(\"N\")\n",
    "    if \"-\" in site:\n",
    "        site.pop(\"-\")\n",
    "\n",
    "    ## get the two most common alleles\n",
    "    if site:\n",
    "        base1 = base2 = 0\n",
    "        comms = site.most_common()\n",
    "        base1 = comms[0][1]\n",
    "        if len(comms) > 1:\n",
    "            base2 = comms[1][1]\n",
    "\n",
    "        ## if site depth after removing Ns, (-s) and third bases is below limit\n",
    "        bidepth = base1 + base2\n",
    "        if bidepth < data.paramsdict[\"mindepth_majrule\"]:\n",
    "            cons = \"N\"\n",
    "\n",
    "        else:\n",
    "            ## if depth > 500 reduce to randomly sampled 500 \n",
    "            if bidepth >= 500: \n",
    "                randomsample = numpy.array(tuple(\"A\"*base1+\"B\"*base2))\n",
    "                numpy.random.shuffle(randomsample)\n",
    "                base1 = list(randomsample[:500]).count(\"A\")\n",
    "                base2 = list(randomsample[:500]).count(\"B\")\n",
    "\n",
    "            ## speedhack: make the base call using a method depending on depth\n",
    "            ## if highdepth and invariable just call the only base\n",
    "            if (bidepth > 10) and (not base2):\n",
    "                cons = comms[0][0]\n",
    "            ## but if variable then use basecaller\n",
    "            else:\n",
    "                cons = basecaller(data, site, base1, base2)\n",
    "    else:\n",
    "        cons = \"N\"\n",
    "    return cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['A', 'A', 'A', 'A', 'T', 'n', 'n', 'n', 'n', 'G', 'G', 'G', 'G'],\n",
       "       ['A', 'A', 'A', 'A', '-', 'n', 'n', 'n', 'n', 'G', 'G', 'G', 'G'],\n",
       "       ['A', 'A', 'A', 'A', '-', 'n', 'n', 'n', 'n', 'G', 'G', 'G', 'G'],\n",
       "       ['A', 'A', 'A', 'A', '-', 'n', 'n', 'n', 'n', 'G', 'G', 'G', 'G'],\n",
       "       ['A', 'A', 'A', 'A', '-', 'n', 'n', 'n', 'n', 'G', 'G', 'G', 'G'],\n",
       "       ['A', 'A', 'A', 'A', '-', 'n', 'n', 'n', 'n', 'G', 'G', 'G', 'G'],\n",
       "       ['A', 'A', 'A', 'A', '-', 'n', 'n', 'n', 'n', 'G', 'G', 'G', 'G']], \n",
       "      dtype='|S1')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'A', 'A', 'A', 'A', 'A', 'A'], \n",
       "      dtype='|S1')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site = arrayed[:, 1]\n",
    "site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newcall(site, data):\n",
    "    \"\"\" prepares stack for making base calls \"\"\"\n",
    "    ## count em\n",
    "    site = Counter(site)\n",
    "\n",
    "    ## remove Ns and (-)s\n",
    "    if \"N\" in site:\n",
    "        site.pop(\"N\")\n",
    "    if \"-\" in site:\n",
    "        site.pop(\"-\")\n",
    "\n",
    "    ## get the two most common alleles\n",
    "    if site:\n",
    "        base1 = base2 = 0\n",
    "        comms = site.most_common()\n",
    "        base1 = comms[0][1]\n",
    "        if len(comms) > 1:\n",
    "            base2 = comms[1][1]\n",
    "\n",
    "        ## if site depth after removing Ns, (-s) and third bases is below limit\n",
    "        bidepth = base1 + base2\n",
    "        if bidepth < data.paramsdict[\"mindepth_majrule\"]:\n",
    "            cons = \"N\"\n",
    "\n",
    "        else:\n",
    "            ## if depth > 500 divide to a number below 500\n",
    "            if bidepth >= 500:\n",
    "                divisor = base1 // 500\n",
    "                base1 //= divisor\n",
    "                base2 //= divisor\n",
    "                #randomsample = numpy.array(tuple(\"A\"*base1+\"B\"*base2))\n",
    "                #numpy.random.shuffle(randomsample)\n",
    "                #base1 = list(randomsample[:500]).count(\"A\")\n",
    "                #base2 = list(randomsample[:500]).count(\"B\")\n",
    "\n",
    "            ## speedhack: make the base call using a method depending on depth\n",
    "            ## if highdepth and invariable just call the only base\n",
    "            if (bidepth > 10) and (not base2):\n",
    "                cons = comms[0][0]\n",
    "            ## but if variable then use basecaller\n",
    "            else:\n",
    "                cons = basecaller(data, site, base1, base2)\n",
    "    else:\n",
    "        cons = \"N\"\n",
    "    return cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basecaller(data, site, base1, base2):\n",
    "    \"\"\" inputs data to binomprobr and gets alleles correctly oriented \"\"\"\n",
    "\n",
    "    ## make statistical base call\n",
    "    if base1+base2 >= data.paramsdict[\"mindepth_statistical\"]:\n",
    "        prob, _, who = binomprobr(base1, base2, data._este, data._esth)\n",
    "        \n",
    "    elif base1+base2 >= data.paramsdict[\"mindepth_majrule\"]:\n",
    "        prob, _, who = simpleconsensus(base1, base2)\n",
    "\n",
    "    else:\n",
    "        LOGGER.error(\"gap in mindepth settings\")\n",
    "\n",
    "    ## if the base could be called with 95% probability\n",
    "    if float(prob) >= 0.95:\n",
    "        if who != \"ab\":\n",
    "            ## site is homozygous\n",
    "            cons = site.most_common(1)[0][0]\n",
    "        else:\n",
    "            ## site is heterozygous\n",
    "            cons = hetero(*[i[0] for i in site.most_common(2)])\n",
    "    else:\n",
    "        cons = \"N\"\n",
    "    return cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def simpleconsensus(base1, base2):\n",
    "    \"\"\"\n",
    "    majority consensus calling for sites with too low of coverage for\n",
    "    statistical calling. Only used with 'lowcounts' option. Returns \n",
    "    the most common base. Returns consistent alphabetical order for ties.\n",
    "    \"\"\"\n",
    "    #qQn = ['aa','bb','ab']\n",
    "    maf = base1/(base1+base2)\n",
    "    return [1.0, maf, 'aa']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binomprobr(base1, base2, error, het):\n",
    "    \"\"\"\n",
    "    given two bases are observed at a site n1 and n2, and the error rate e, the\n",
    "    probability the site is truly aa,bb,ab is calculated using binomial \n",
    "    distribution as in Li_et al 2009, 2011, and if coverage > 500, 500 \n",
    "    dereplicated reads were randomly sampled.\n",
    "    \"\"\"\n",
    "    ## major allele freq\n",
    "    mjaf = base1/float(base1+base2)\n",
    "    prior_homo = ((1.-het)/2.)\n",
    "    prior_het = het\n",
    "\n",
    "    ## get probabilities. Note, b/c only allow two bases, base2 == sum-base1\n",
    "    hetro = scipy.misc.comb(base1+base2, base1)/(2.**(base1+base2))\n",
    "    homoa = scipy.stats.binom.pmf(base2, base1+base2, error)\n",
    "    homob = scipy.stats.binom.pmf(base1, base1+base2, error)\n",
    "\n",
    "    ## calculate probs\n",
    "    homoa *= prior_homo\n",
    "    homob *= prior_homo\n",
    "    hetro *= prior_het\n",
    "\n",
    "    ## return \n",
    "    probabilities = [homoa, homob, hetro]\n",
    "    genotypes = ['aa', 'bb', 'ab']\n",
    "    bestprob = max(probabilities)/float(sum(probabilities))\n",
    "\n",
    "    return [bestprob, mjaf, genotypes[probabilities.index(max(probabilities))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removerepeats(consens, arrayed):\n",
    "    \"\"\" Checks for interior Ns in consensus seqs and removes those that are at\n",
    "    low depth, here defined as less than 1/3 of the average depth. The prop 1/3\n",
    "    is chosen so that mindepth=6 requires 2 base calls that are not in [N,-].\n",
    "    \"\"\"\n",
    "\n",
    "    ## default trim no edges\n",
    "    consens = \"\".join(consens).replace(\"-\", \"N\")\n",
    "    edges = [None, None]\n",
    "\n",
    "    ## trim from left else index starts at zero\n",
    "    lcons = len(consens)\n",
    "    consens = consens.lstrip(\"N\")\n",
    "    edges[0] = lcons - len(consens)\n",
    "\n",
    "    ## trim from right if nonzero\n",
    "    lcons = len(consens)\n",
    "    consens = consens.rstrip(\"N\")\n",
    "    if lcons - len(consens):\n",
    "        edges[1] = -1*(lcons - len(consens))\n",
    "\n",
    "    ## trim same from arrayed\n",
    "    arrayed = arrayed[:, edges[0]:edges[1]]\n",
    "\n",
    "    ## what is the total site coverage\n",
    "    totdepth = arrayed.shape[0]\n",
    "    mindepth = max(1, totdepth // 3)\n",
    "\n",
    "    ## test across N-called sites\n",
    "    nsites = [i for (i, j) in enumerate(consens) if j == \"N\"]\n",
    "\n",
    "    ## get column counts of Ns and -s\n",
    "    ndepths = np.sum(arrayed == 'N', axis=0) \n",
    "    idepths = np.sum(arrayed == '-', axis=0)\n",
    "\n",
    "    ## find sites to remove\n",
    "    ridx = []\n",
    "    for nsite in nsites:\n",
    "        ## If not at least mindepth non (N-) char at site, then remove\n",
    "        if (idepths[nsite]+ndepths[nsite]) > mindepth:\n",
    "            ridx.append(nsite)\n",
    "    \n",
    "    ## remove repeat sites from shortcon and stacked\n",
    "    ## If consens is all N's this will raise a ValueError which \n",
    "    ## consensus() will catch and then pass over this sample.\n",
    "    keeps, consens = zip(*[(i, j) for (i, j) in enumerate(consens) \\\n",
    "                        if i not in ridx])\n",
    "\n",
    "    consens = \"\".join(list(consens))\n",
    "    arrayed = arrayed[:, list(keeps)]\n",
    "\n",
    "    return np.array(consens), arrayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_repeats(consens, arrayed):\n",
    "    \"\"\" Checks for interior Ns in consensus seqs and removes those that are at\n",
    "    low depth, here defined as less than 1/3 of the average depth. The prop 1/3\n",
    "    is chosen so that mindepth=6 requires 2 base calls that are not in [N,-].\n",
    "    \"\"\"\n",
    "\n",
    "    ## default trim no edges\n",
    "    consens = \"\".join(consens).replace(\"-\", \"N\")\n",
    "    edges = [None, None]\n",
    "\n",
    "    ## trim from left else index starts at zero\n",
    "    lcons = len(consens)\n",
    "    consens = consens.lstrip(\"N\")\n",
    "    edges[0] = lcons - len(consens)\n",
    "\n",
    "    ## trim from right if nonzero\n",
    "    lcons = len(consens)\n",
    "    consens = consens.rstrip(\"N\")\n",
    "    if lcons - len(consens):\n",
    "        edges[1] = -1*(lcons - len(consens))\n",
    "\n",
    "    ## trim same from arrayed\n",
    "    consens = np.array(list(consens))\n",
    "    arrayed = arrayed[:, edges[0]:edges[1]]\n",
    "\n",
    "    ## get column counts of Ns and -s\n",
    "    ndepths = np.sum(arrayed == 'N', axis=0) \n",
    "    idepths = np.sum(arrayed == '-', axis=0)\n",
    "\n",
    "    ## get proportion of bases that are N- at each site\n",
    "    nons = ((ndepths + idepths) / float(arrayed.shape[0])) >= 0.75\n",
    "    ## boolean of whether base was called N\n",
    "    isn = consens == \"N\"\n",
    "    ## make ridx\n",
    "    ridx = nons * isn\n",
    "\n",
    "    ## apply filter\n",
    "    consens = consens[~ridx]\n",
    "    arrayed = arrayed[:, ~ridx]\n",
    "    \n",
    "    return consens, arrayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True False]\n",
      "[ True  True False False]\n",
      "[False  True False False]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0, 1, 1, 0], dtype=np.bool)\n",
    "b = np.array([1, 1, 0, 0], dtype=np.bool)\n",
    "print a\n",
    "print b\n",
    "print a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True,  True,  True,  True,\n",
       "       False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndepths = np.sum(arrayed==\"n\", axis=0)\n",
    "idepths = np.sum(arrayed==\"i\", axis=0)\n",
    "\n",
    "nons = (ndepths+idepths) / float(arrayed.shape[0])\n",
    "nons > 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'A', 'A', 'A', 'n', 'n', 'n', 'n', 'G', 'G', 'G', 'G'], \n",
       "      dtype='|S1')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rconsens, rarray = remove_repeats(consens, arrayed)\n",
    "rconsens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rconsens[rconsens==\"N\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.56 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "10000 loops, best of 3: 63.8 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "removerepeats(consens, arrayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data._este = data.stats.error_est.mean()\n",
    "data._esth = data.stats.hetero_est.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 4.26 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "np.apply_along_axis(basecall, 0, arrayed, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 3.88 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "np.apply_along_axis(newcall, 0, arrayed, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'A', 'A', 'A', 'N', 'n', 'n', 'n', 'n', 'G', 'G', 'G', 'G'], \n",
       "      dtype='|S1')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consens = np.apply_along_axis(newcall, 0, arrayed, data)\n",
    "consens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.00 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "100000 loops, best of 3: 3 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "[i for (i,j) in enumerate(consens) if j in \"RKSYWM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 loops, best of 3: 17.3 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "[np.where(consens==i) for i in \"RKSYWM\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
