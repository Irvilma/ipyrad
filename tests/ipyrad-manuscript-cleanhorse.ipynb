{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembling simulated and empirical data\n",
    "\n",
    "This Jupyter notebook provides a completely reproducible record of all the assembly analyses for the ipyrad manuscript. In this notebook we assemble multiple data sets using five different software programs: ipyrad, pyrad, stacks, aftrRAD, and dDocent. We include executable code to download and install each program, to download each data set, and to run each data set through each program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook SSH Tunneling\n",
    "This notebook was executed on the Yale farnam cluster with access to 40 compute cores on a single node. We performed local I/O in the notebook using SSH Tunneling as described in the ipyrad Documentation (http://ipyrad.readthedocs.io/HPC_Tunnel.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set the default directories for exec and data. \n",
    "WORK_DIR = \"/home/deren/manuscript-analysis\"\n",
    "\n",
    "## create dirs for the data \n",
    "EMPERICAL_DATA_DIR = os.path.join(WORK_DIR, \"example_empirical_rad\")\n",
    "SIMULATION_DATA_DIR = os.path.join(WORK_DIR, \"simulated_data\")\n",
    "\n",
    "## create dirs for results from each software\n",
    "IPYRAD_DIR = os.path.join(WORK_DIR, \"assembly-ipyrad\")\n",
    "PYRAD_DIR = os.path.join(WORK_DIR, \"assembly-pyrad\")\n",
    "STACKS_DIR = os.path.join(WORK_DIR, \"assembly-stacks\")\n",
    "AFTRRAD_DIR = os.path.join(WORK_DIR, \"assembly-aftrRAD\")\n",
    "DDOCENT_DIR = os.path.join(WORK_DIR, \"assembly-dDocent\")\n",
    "\n",
    "## (empirical data dir will be created for us when we untar it)\n",
    "for dir in [WORK_DIR, IPYRAD_DIR, PYRAD_DIR, STACKS_DIR, AFTRRAD_DIR, DDOCENT_DIR]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        \n",
    "## Simulated data directories\n",
    "SIMNO = os.path.join(WORK_DIR, \"simno\")\n",
    "SIMLO = os.path.join(WORK_DIR, \"simlo\")\n",
    "SIMHI = os.path.join(WORK_DIR, \"simhi\")\n",
    "SIMLA = os.path.join(WORK_DIR, \"simla\")\n",
    "\n",
    "## create directories\n",
    "for dir in [SIMNO, SIMLO, SIMHI, SIMLA]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "## subdirectories for results\n",
    "IPYRAD_OUTPUT = os.path.join(IPYRAD_DIR, \"REALDATA\")\n",
    "PYRAD_OUTPUT = os.path.join(PYRAD_DIR, \"REALDATA\")\n",
    "STACKS_OUTPUT = os.path.join(STACKS_DIR, \"REALDATA\")\n",
    "STACKS_GAP_OUT = os.path.join(STACKS_OUTPUT, \"gapped\")\n",
    "STACKS_UNGAP_OUT = os.path.join(STACKS_OUTPUT, \"ungapped\")\n",
    "STACKS_DEFAULT_OUT = os.path.join(STACKS_OUTPUT, \"default\")\n",
    "AFTRRAD_OUTPUT = os.path.join(AFTRRAD_DIR, \"REALDATA\")\n",
    "DDOCENT_OUTPUT = os.path.join(DDOCENT_DIR, \"REALDATA\")\n",
    "\n",
    "## Make the empirical output directories if they don't already exist\n",
    "for dir in [IPYRAD_OUTPUT, PYRAD_OUTPUT, STACKS_OUTPUT,\n",
    "            STACKS_GAP_OUT, STACKS_UNGAP_OUT, STACKS_DEFAULT_OUT,\n",
    "            AFTRRAD_OUTPUT, DDOCENT_OUTPUT]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        \n",
    "## in case we're not in the workdir, get there\n",
    "os.chdir(WORK_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the empirical RAD *Pedicularis* data set\n",
    "\n",
    "This is a RAD data set for 13 taxa from Eaton and Ree (2013) [open access link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3739883/pdf/syt032.pdf). Here we grab the demultiplexed fastq data from a public Dropbox link, but the same data is also hosted on the NCBI SRA as SRP021469.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_empirical_rad/\n",
      "example_empirical_rad/38362_rex.fastq.gz\n",
      "example_empirical_rad/32082_przewalskii.fastq.gz\n",
      "example_empirical_rad/40578_rex.fastq.gz\n",
      "example_empirical_rad/30686_cyathophylla.fastq.gz\n",
      "example_empirical_rad/39618_rex.fastq.gz\n",
      "example_empirical_rad/41954_cyathophylloides.fastq.gz\n",
      "example_empirical_rad/41478_cyathophylloides.fastq.gz\n",
      "example_empirical_rad/33588_przewalskii.fastq.gz\n",
      "example_empirical_rad/35855_rex.fastq.gz\n",
      "example_empirical_rad/35236_rex.fastq.gz\n",
      "example_empirical_rad/29154_superba.fastq.gz\n",
      "example_empirical_rad/30556_thamno.fastq.gz\n",
      "example_empirical_rad/33413_thamno.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "## the location of the data \n",
    "dataloc=\"https://dl.dropboxusercontent.com/u/2538935/example_empirical_rad.tar.gz\"\n",
    "\n",
    "## grab data from the public link (uses an upper-case o argument, not a zero).\n",
    "curl -LkO $dataloc > /dev/null 2>&1\n",
    "\n",
    "## the tar command decompresses the data directory\n",
    "tar -xvzf example_empirical_rad.tar.gz\n",
    "\n",
    "## remove the large tar file\n",
    "rm example_empirical_rad.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the empirical *Heliconius* reference genomeÂ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./._Hmel2\n",
      "Hmel2/\n",
      "Hmel2/._annotation\n",
      "Hmel2/annotation/\n",
      "Hmel2/._ChangeLog.txt\n",
      "Hmel2/ChangeLog.txt\n",
      "Hmel2/._Hmel2.fa\n",
      "Hmel2/Hmel2.fa\n",
      "Hmel2/._Hmel_mtDNA.fa\n",
      "Hmel2/Hmel_mtDNA.fa\n",
      "Hmel2/._maps\n",
      "Hmel2/maps/\n",
      "Hmel2/._README.txt\n",
      "Hmel2/README.txt\n",
      "Hmel2/._repeats\n",
      "Hmel2/repeats/\n",
      "Hmel2/._transfer\n",
      "Hmel2/transfer/\n",
      "Hmel2/transfer/._Hmel1-1_Hmel2.chain\n",
      "Hmel2/transfer/Hmel1-1_Hmel2.chain\n",
      "Hmel2/transfer/._Hmel2_broken.gff\n",
      "Hmel2/transfer/Hmel2_broken.gff\n",
      "Hmel2/transfer/._Hmel2_removed.gff\n",
      "Hmel2/transfer/Hmel2_removed.gff\n",
      "Hmel2/transfer/._Hmel2_transfer_new.tsv\n",
      "Hmel2/transfer/Hmel2_transfer_new.tsv\n",
      "Hmel2/transfer/._Hmel2_transfer_old.tsv\n",
      "Hmel2/transfer/Hmel2_transfer_old.tsv\n",
      "Hmel2/repeats/._Hmel.all.named.final.1-31.lib\n",
      "Hmel2/repeats/Hmel.all.named.final.1-31.lib\n",
      "Hmel2/maps/._Hmel2_chromosome_linkage.tsv\n",
      "Hmel2/maps/Hmel2_chromosome_linkage.tsv\n",
      "Hmel2/maps/._Hmel2_chromosomes.agp\n",
      "Hmel2/maps/Hmel2_chromosomes.agp\n",
      "Hmel2/maps/._Hmel2_scaffold_linkage.tsv\n",
      "Hmel2/maps/Hmel2_scaffold_linkage.tsv\n",
      "Hmel2/maps/._Hmel2_scaffolds.agp\n",
      "Hmel2/maps/Hmel2_scaffolds.agp\n",
      "Hmel2/annotation/._Hmel2.gff\n",
      "Hmel2/annotation/Hmel2.gff\n",
      "Hmel2/annotation/._Hmel2_cds.fa\n",
      "Hmel2/annotation/Hmel2_cds.fa\n",
      "Hmel2/annotation/._Hmel2_proteins.fa\n",
      "Hmel2/annotation/Hmel2_proteins.fa\n",
      "Hmel2/annotation/._Hmel_mtDNA.gff\n",
      "Hmel2/annotation/Hmel_mtDNA.gff\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## Fetch the heliconius genome and rad data from\n",
    "## Davey, John W., et al. \"Major improvements to the Heliconius melpomene \n",
    "## genome assembly used to confirm 10 chromosome fusion events in 6 \n",
    "## million years of butterfly evolution.\" G3: Genes| Genomes| Genetics 6.3\n",
    "## (2016): 695-708.\n",
    "##curl -LkO http://butterflygenome.org/sites/default/files/Hmel2-0_Release_20160201.tgz\n",
    "\n",
    "## location of the data\n",
    "dataloc=\"http://butterflygenome.org/sites/default/files/Hmel2-0_Release_20151013.tgz\"\n",
    "\n",
    "## grab the data from the public link\n",
    "curl -LkO $dataloc > /dev/null 2>&1\n",
    "\n",
    "## untar the files\n",
    "tar -zxvf Hmel2-0_Release_20151013.tgz\n",
    "\n",
    "## remove the tar file\n",
    "rm Hmel2-0_Release_20151013.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install all the software "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First install a new isolated miniconda directory\n",
    "\n",
    "We could create a separate environment in an existing conda installation, but this is simpler since it works for users whether they have conda installed or not, and we can easily remove all the software at the end when we are done by simply deleting the miniconda directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conda: /home/deren/manuscript-analysis/miniconda/bin/conda\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$WORK_DIR\"\n",
    "\n",
    "## path to the installer download\n",
    "miniconda=\"https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh\"\n",
    "\n",
    "## Fetch the latest miniconda installer\n",
    "curl -LkO $miniconda > /dev/null 2>&1\n",
    "\n",
    "## Install miniconda silently to the work directory.\n",
    "## -b = \"batch\" mode, -f = force overwrite, -p = install dir\n",
    "bash Miniconda-latest-Linux-x86_64.sh -b -f -p $1/miniconda > /dev/null 2>&1 \n",
    "\n",
    "## update conda, if this fails your other conda installation may be getting in \n",
    "## the way. Try temporarily removing your ~/.condarc file.\n",
    "export PATH=\"$1/miniconda/bin:$PATH\"\n",
    "conda update -y conda > /dev/null 2>&1 \n",
    "echo 'conda: '$(which conda)\n",
    "\n",
    "## remove the install file\n",
    "rm Miniconda-latest-Linux-x86_64.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install ipyrad (Eaton & Overcast 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipyrad: /home/deren/manuscript-analysis/miniconda/bin/ipyrad\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$WORK_DIR\"\n",
    "\n",
    "## ensure we are using the workdir conda\n",
    "export PATH=\"$1/miniconda/bin:$PATH\"; export \"WORK_DIR=$1\"\n",
    "\n",
    "## installs the latest release (silently)\n",
    "conda install -y -c ipyrad ipyrad > /dev/null 2>&1  \n",
    "echo 'ipyrad: '$(which ipyrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install stacks (Catchen)\n",
    "\n",
    "For this notebook we are using linux which is a bit easier than installing Stacks on OSX. Stacks is picky about where stuff installs to. If you don't have permission to install to /usr/local (most HPC systems) then you need to provide the --prefix argument to ./configure as we do here to install it locally (into miniconda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacks: /home/deren/manuscript-analysis/miniconda/bin/process_radtags\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$WORK_DIR\"\n",
    "\n",
    "## ensure we are using the workdir conda\n",
    "export PATH=\"$1/miniconda/bin:$PATH\"; export \"WORK_DIR=$1\"\n",
    "\n",
    "## installs the latest release (silently)\n",
    "conda install -y -c bioconda stacks > /dev/null 2>&1  \n",
    "echo 'stacks: '$(which process_radtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install ddocent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dDocent: /home/deren/manuscript-analysis/miniconda/bin/dDocent\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$WORK_DIR\"\n",
    "\n",
    "## ensure we're in the right conda\n",
    "export PATH=\"$1/miniconda/bin:$PATH\"; export \"WORK_DIR=$1\"\n",
    "\n",
    "## install all of the ddocent reqs\n",
    "conda install -y -c bioconda ddocent=2.2.4 > /dev/null 2>&1  \n",
    "echo 'dDocent: '$(which dDocent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install pyrad (Eaton 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "muscle: /home/deren/manuscript-analysis/miniconda/bin/muscle\n",
      "vsearch: /home/deren/manuscript-analysis/miniconda/bin/vsearch\n",
      "pyrad: /home/deren/manuscript-analysis/miniconda/bin/pyrad\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$WORK_DIR\"\n",
    "\n",
    "## ensure we are using the workdir conda\n",
    "export PATH=\"$1/miniconda/bin:$PATH\"; export \"WORK_DIR=$1\"\n",
    "\n",
    "## Should be unnecessary because numpy and scipy already installed by conda\n",
    "conda install numpy scipy > /dev/null 2>&1\n",
    "\n",
    "## get muscle binary\n",
    "muscle=\"http://www.drive5.com/muscle/downloads3.8.31/muscle3.8.31_i86linux64.tar.gz\"\n",
    "curl -LkO $muscle > /dev/null 2>&1\n",
    "tar -xvzf muscle*.tar.gz > /dev/null 2>&1\n",
    "mv muscle3.8.31_i86linux64 $WORK_DIR/miniconda/bin/muscle\n",
    "rm muscle3.8.31_i86linux64.tar.gz\n",
    "echo 'muscle: '$(which muscle)\n",
    "\n",
    "## Download and install vsearch\n",
    "vsearch=\"https://github.com/torognes/vsearch/releases/download/v2.0.3/vsearch-2.0.3-linux-x86_64.tar.gz\"\n",
    "curl -LkO $vsearch > /dev/null 2>&1\n",
    "tar xzf vsearch-2.0.3-linux-x86_64.tar.gz > /dev/null 2>&1\n",
    "mv vsearch-2.0.3-linux-x86_64/bin/vsearch $WORK_DIR/miniconda/bin/vsearch\n",
    "rm -r vsearch-2.0.3-linux-x86_64/ vsearch-2.0.3-linux-x86_64.tar.gz\n",
    "echo 'vsearch: '$(which vsearch)\n",
    "\n",
    "## Fetch pyrad source from git repository \n",
    "if [ ! -d ./pyrad-git ]; then\n",
    "  git clone https://github.com/dereneaton/pyrad.git pyrad-git > /dev/null 2>&1\n",
    "fi;\n",
    "\n",
    "## and install to conda using pip\n",
    "cd ./pyrad-git\n",
    "pip install -e . > /dev/null 2>&1\n",
    "cd ..\n",
    "echo 'pyrad: '$(which pyrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate data\n",
    "We will use simrrls to generate some simulated RAD-seq data for testing. This is a program that was written by Deren Eaton and is available on github: github.com/dereneaton/simrrls.git. simrrls requires the python egglib module, which is a pain to install in full, but we only need the simulation aspects of it, which are fairly easy to install. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simrrls: /home/deren/manuscript-analysis/miniconda/bin/simrrls\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$WORK_DIR\"\n",
    "\n",
    "## ensure we are using the new conda\n",
    "export PATH=\"$1/miniconda/bin:$PATH\"; export \"WORK_DIR=$1\"\n",
    "\n",
    "## install gsl\n",
    "conda install gsl -y > /dev/null 2>&1\n",
    "\n",
    "## get egglib cpp and py\n",
    "eggcpp=\"http://mycor.nancy.inra.fr/egglib/releases/2.1.11/egglib-cpp-2.1.11.tar.gz\"\n",
    "curl -LkO $eggcpp > /dev/null 2>&1\n",
    "tar -xzvf egglib-cpp-*.tar.gz > /dev/null 2>&1\n",
    "cd egglib-cpp-*/\n",
    "sh ./configure --prefix=$WORK_DIR/miniconda/ > /dev/null 2>&1\n",
    "make > /dev/null 2>&1\n",
    "make install > /dev/null 2>&1\n",
    "cd ..\n",
    "\n",
    "## install py module\n",
    "eggpy=\"http://mycor.nancy.inra.fr/egglib/releases/2.1.11/egglib-py-2.1.11.tar.gz\"\n",
    "curl -LkO $eggpy > /dev/null 2>&1\n",
    "tar -xvzf egglib-py-2.1.11.tar.gz > /dev/null 2>&1\n",
    "cd egglib-py-2.1.11/\n",
    "python setup.py build --prefix=$WORK_DIR/miniconda > /dev/null 2>&1\n",
    "python setup.py install --prefix=$WORK_DIR/miniconda > /dev/null 2>&1\n",
    "cd ..\n",
    "\n",
    "## install simrrls\n",
    "if [ ! -d simrrls ] ; then\n",
    "  git clone https://github.com/dereneaton/simrrls.git\n",
    "fi\n",
    "cd simrrls\n",
    "pip install -e . > /dev/null 2>&1\n",
    "cd ..\n",
    "echo 'simrrls: '$(which simrrls)\n",
    "\n",
    "## cleanup\n",
    "rm -r egglib-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Simulating different RAD-datasets\n",
    "\n",
    "Both pyRAD and stacks have undergone a lot of work since the original pyrad analysis. Because improvements have been made we want to test performance of all the current pipelines and be able to compare current to past performance. We'll follow the original pyRAD manuscript analysis (Eaton 2013) by simulating modest sized datasets with variable amounts of indels. We'll also simulate one much larger dataset. Also, because stacks has since included an option for handling gapped analysis we'll test both gapped and ungapped assembly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning simrrls indel parameter\n",
    "\n",
    "The -I parameter for simrrls has changed since the initial pyrad manuscript, so the we had to explore new values for this parameter that will approximate the number of indels we are after. I figured out a way to run simrrls and pipe the output to muscle to get a quick idea of the indel variation for different params. This gives a good idea of how many indel bearing seqs are generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simrrls -n 1 -L 1 -I 1 -r1 $RANDOM 2>&1 | \\\n",
    "#                             grep 0 -A 1 | \\\n",
    "#                             tr '@' '>' | \\\n",
    "#                             muscle | grep T | head -n 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in {1..50}; \n",
    "#   do simrrls -n 1 -L 1 -I .05 -r1 $RANDOM 2>&1 | \\\n",
    "#                                   grep 0 -A 1 | \\\n",
    "#                                   tr '@' '>' | \\\n",
    "#                                   muscle |  \\\n",
    "#                                   grep T | \\\n",
    "#                                   head -n 40 >> rpt.txt;\n",
    "# done\n",
    "# grep \"-\" rpt.txt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From experimentation:\n",
    "\n",
    "-I value -- %loci w/ indels\n",
    "\n",
    "    0.02 -- ~10%\n",
    "    0.05 -- ~15%\n",
    "    0.10 -- ~25%\n",
    "\n",
    "The simulated data will live in these directories:\n",
    "\n",
    "    SIM_NO_DIR = WORK_DIR/simulated_data/simno\n",
    "    SIM_LO_DIR = WORK_DIR/simulated_data/simlo\n",
    "    SIM_HI_DIR = WORK_DIR/simulated_data/simhi\n",
    "    SIM_LARGE_DIR = WORK_DIR/simulated_data/simlarge\n",
    "\n",
    "Timing:\n",
    "\n",
    "    10K loci -- ~8MB -- ~ 2 minutes\n",
    "    100K loci -- ~80MB -- ~ 20 minutes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize directories for sim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "force = True\n",
    "\n",
    "## Directories for the simulation data\n",
    "SIM_NO_DIR=os.path.join(SIMULATION_DATA_DIR, \"simno\")\n",
    "SIM_LO_DIR=os.path.join(SIMULATION_DATA_DIR, \"simlo\")\n",
    "SIM_HI_DIR=os.path.join(SIMULATION_DATA_DIR, \"simhi\")\n",
    "SIM_LARGE_DIR=os.path.join(SIMULATION_DATA_DIR, \"simlarge\")\n",
    "\n",
    "for idir in [SIMULATION_DATA_DIR, SIM_NO_DIR, SIM_LO_DIR,\\\n",
    "            SIM_HI_DIR, SIM_LARGE_DIR]:\n",
    "    if force and os.path.exists(idir):\n",
    "        shutil.rmtree(idir)\n",
    "    if not os.path.exists(idir):\n",
    "        os.makedirs(idir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call *simrrls* to simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$WORK_DIR\"\n",
    "\n",
    "## fix path\n",
    "export PATH=$1/miniconda/bin:$PATH; \n",
    "\n",
    "## call simrrls (No INDELS)\n",
    "simrrls -o \"simno/simno\" -ds 2 -L 10000 -I 0 \n",
    "\n",
    "## call simrrls (Low INDELS)\n",
    "simrrls -o \"simlo/simlo\" -ds 2 -L 10000 -I 0.02\n",
    "\n",
    "## call simrrls (High INDELS) \n",
    "simrrls -o \"simhi/simhi\" -ds 2 -L 10000 -I 0.05\n",
    "\n",
    "## call simrls on Large data set (this takes a few hours)\n",
    "## (30x12=360 Individuals at 100K loci)\n",
    "simrrls -o \"simla/Big_i360_L100K\" -ds 0 -L 100000 -n 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Demultiplex all four libraries\n",
    "\n",
    "We will start each analysis from the demultiplexed data files, since this is commonly what's available when data are downloaded from NCBI. We use ipyrad to demultiplex the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ipyrad as ip\n",
    "\n",
    "for idir in [\"no\", \"lo\", \"hi\", \"la\"]:\n",
    "    ## demultiplex the library\n",
    "    name = \"sim{}\".format(idir)\n",
    "    data = ip.Assembly(name)\n",
    "    data.set_params(\"project_dir\", name)\n",
    "    data.set_params(\"raw_fastq_path\", name+\"/*.gz\")\n",
    "    data.set_params(\"barcodes_path\", name+\"/*barcodes.txt\")\n",
    "    data.run(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ipyrad : simulated data assembly\n",
    "Results  \n",
    "Data set \tCores \tLoci \tTime  \n",
    "simno \t4 \t10000 \t13:58  \n",
    "simlo \t4 \t10000 \t15:17  \n",
    "simhi \t4 \t10000 \t13:58  \n",
    "simla \t4 \t100000 \t13:38  \n",
    "simla \t80 \t100000 \t...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.15'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyrad as ip\n",
    "ip.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1 \n",
    "## this records time to run the code in this cell once\n",
    "\n",
    "## create Assembly\n",
    "data1 = ip.Assembly(\"simno\")\n",
    "\n",
    "## set params, including path to store data in ipyrad results dir\n",
    "data1.set_params(\"project_dir\", os.path.join(IPYRAD_DIR, \"SIMDATA\"))\n",
    "data1.set_params(\"sorted_fastq_path\", \"simno/simno_fastqs/*.gz\")\n",
    "data1.set_params('max_low_qual_bases', 4)\n",
    "data1.set_params('filter_min_trim_len', 69)\n",
    "data1.set_params('max_Ns_consens', (99,99))\n",
    "data1.set_params('max_Hs_consens', (99,99))\n",
    "data1.set_params('max_SNPs_locus', (100, 100))\n",
    "data1.set_params('min_samples_locus', 2)\n",
    "data1.set_params('max_Indels_locus', (99,99))\n",
    "data1.set_params('max_shared_Hs_locus', 99)\n",
    "data1.set_params('trim_overhang', (2,2,2,2))\n",
    "\n",
    "## run ipyrad steps 1-7\n",
    "data1.run(\"1234567\", show_cluster=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
