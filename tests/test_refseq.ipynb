{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _ipyrad_ testing tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "Import _ipyrad_ and remove previous test files if they are already present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ipyrad:H4CKERZ-mode: __loglevel__ = INFO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.66\n"
     ]
    }
   ],
   "source": [
    "## import modules\n",
    "import ipyrad as ip      ## for RADseq assembly\n",
    "print ip.__version__     ## print version\n",
    "\n",
    "## clear data from test directory if it already exists\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "#if os.path.exists(\"./test_refseq/\"):\n",
    "#    shutil.rmtree(\"./test_refseq/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This is useful during development since IPython\n",
    "## seems to want to re-use old .pyc files, \n",
    "## though even this doesn't always work...\n",
    "import IPython.lib.deepreload\n",
    "import __builtin__\n",
    "from IPython.lib import deepreload\n",
    "__builtin__.reload = deepreload.reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize smalt (index reference sequence)\n",
    "This is preparation for indexing. It only ever needs to be done once so shoud be tested during initialization.\n",
    "\n",
    "`smalt index zf-ref ../zf/zf.sm.fa`\n",
    "\n",
    "There is an optional -s flag that could improve mapping accuracy. Consider the best default, probably not worth letting people pass it in, if they want to mess with it they can index their own reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hack the binary paths cuz the current egg doesn't have them in it\n",
    "#data1.muscle=\n",
    "#data1.vsearch\n",
    "#data1.smalt\n",
    "\n",
    "# Reference sequence directory (gzipped fasta files)\n",
    "# TODO: set this as a parameter\n",
    "# e.g., data1.set_params('refseq', \"./data/zf.fa.gz\")\n",
    "\n",
    "# TODO: push this example file to the data/ dir \n",
    "REFSEQ = \"./data/zf.fa.gz\"\n",
    "\n",
    "# Set the step size to 4 (default is 13)\n",
    "# This will slow down read mapping, but increase accuracy\n",
    "SMALT_INDEX_FLAGS = \" -s 4 \"\n",
    "\n",
    "# TODO: create and link a dir/ to the Assembly object for the reference data files\n",
    "data1.dirs.reference = '...'\n",
    "\n",
    "# TODO: create and link index files to Sample objects\n",
    "data1.samples['1A_0'].files.index_smi = '...'\n",
    "data1.samples['1A_0'].files.index_sma = '...'\n",
    "\n",
    "# Test if reference sequence is already indexed\n",
    "# Only index if the .smi and .sma files don't exist, saves lots of time\n",
    "if not os.path.isfile( REFSEQ+\".smi\" ):\n",
    "    # smalt indexing will create two files called REFSEQ.smi and .sma\n",
    "    # in the same directory as the reference sequence. \n",
    "    cmd = data1.smalt + \" index \" + SMALT_INDEX_FLAGS + REFSEQ + \" \" + REFSEQ\n",
    "    print cmd\n",
    "    subprocess.check_call(cmd, shell=True,\n",
    "                            stderr=subprocess.STDOUT,\n",
    "                            stdout=subprocess.PIPE)\n",
    "    #output = subprocess.check_output( \" \".join(cmd), shell=True)\n",
    "else:\n",
    "    print \"Reference sequence index exists\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembly and Sample objects\n",
    "\n",
    "Assembly and Sample objects are used by _ipyrad_ to access data stored on disk and to manipulate it. Each biological sample in a data set is represented in a Sample object, and these Samples are stored inside Assembly objects. The Assembly object contains functions to assemble the data, and stores a log of all steps performed and the resulting statistics of those steps. Assembly objects can be copied or merged to allow branching events where different parameters are applied to assemblies. \n",
    "\n",
    "To create an Assembly object call ip.Assembly and pass it a name for the data set. We could imagine that we planned to assemble and later combine data from multiple sequencing runs, but before combining them each group of samples has to be analyzed under a different set of parameters. As an example, we could call two data sets \"2014_data\" and \"2015_data\". These initially do not contain any Samples. Sample objects are created either by linking fastq files to the Assembly object or by running step 1 to demultiplex raw data files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ipyrad.core.parallel:Local connection to 4 engines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Assembly object `2014_data` created\n",
      "ipyparallel setup: Local connection to 4 engines.\n",
      "New Assembly object `2015_data` created\n"
     ]
    }
   ],
   "source": [
    "## create an Assembly object called data1. \n",
    "## It takes an 'test'\n",
    "data1 = ip.Assembly(\"2014_data\")\n",
    "data2 = ip.Assembly(\"2015_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying assembly parameters\n",
    "All of the parameter settings are linked to an Assembly object, which has a set of default parameters when it is created. These can be viewed using the `get_params()` function. To get more detailed information about all paramteres use `ip.get_params_info()` or to select a single parameter use `ip.get_params_info(3)`. Assembly objects have a function `set_params()` that can be used to modify parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Reference sequence file not found. This must be an absolute path (/home/wat/ipyrad/data/referece.gz) or a path relative to the directory where you're running ipyrad (./data/reference.gz). ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b5a893a02155>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rad'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/Volumes/WorkDrive/ipyrad/refhacking/MusChr1.fa'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m## print the new parameters to screen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/deren/Documents/ipyrad/ipyrad/core/assembly.py\u001b[0m in \u001b[0;36mset_params\u001b[1;34m(self, param, newvalue)\u001b[0m\n\u001b[0;32m    645\u001b[0m             \u001b[1;34m\"Reference sequence file not found. This must be an absolute path \"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m             \u001b[1;33m+\u001b[0m\u001b[1;34m\"(/home/wat/ipyrad/data/referece.gz) or a path relative to the \"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m             +\"directory where you're running ipyrad (./data/reference.gz). \")\n\u001b[0m\u001b[0;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparamsdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reference_sequence'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfullrawpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[28] set to \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfullrawpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Reference sequence file not found. This must be an absolute path (/home/wat/ipyrad/data/referece.gz) or a path relative to the directory where you're running ipyrad (./data/reference.gz). "
     ]
    }
   ],
   "source": [
    "## modify parameters for this Assembly object\n",
    "data1.set_params(1, \"./test_refseq\")\n",
    "data1.set_params(2, \"./data/sim_rad_test_R1_.fastq.gz\")\n",
    "data1.set_params(3, \"./data/sim_rad_test_barcodes.txt\")\n",
    "data1.set_params(4, \"./test/test_refseq/2014_data_fastqs/\")\n",
    "data1.set_params(7, 3)\n",
    "data1.set_params(10, 'rad')\n",
    "data1.set_params(28, '/Volumes/WorkDrive/ipyrad/refhacking/MusChr1.fa')\n",
    "\n",
    "## print the new parameters to screen\n",
    "data1.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting data assembly and Sample objects\n",
    "If the data are already demultiplexed then fastq files can be linked directly to the Data object, which in turn will create Sample objects for each fastq file (or pair of fastq files for paired data). The files may be gzip compressed. If the data are not demultiplexed then you will have to run the step1 function below to demultiplex the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This would link fastq files from the 'sorted_fastq_path' if present\n",
    "## Here it does nothing b/c there are no files in the sorted_fastq_path\n",
    "data1.link_fastqs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Demultiplex the raw data files\n",
    "This uses the barcodes information to demultiplex reads in data files found in the 'raw_fastq_path'. It will create a Sample object for each sample that will be stored in the Assembly object. The state of each sample will be set to 1, meaning that the sample has completed step 1 of the _ipyrad_ assembly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## run step 1 to demultiplex the data\n",
    "data1.step1(preview=1)\n",
    "\n",
    "## print the results for each Sample in data1\n",
    "print data1.stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Filter reads \n",
    "If for some reason we wanted to execute on just a subsample of our data, we could do this by selecting only certain samples to call the `step2` function on. Because `step2` is a function of `data`, it will always execute with the parameters that are linked to `data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## run step 1 to demultiplex the data\n",
    "data1.step1()\n",
    "\n",
    "## print the results for each Sample in data1\n",
    "print data1.stats\n",
    "wat = data1.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Filter reads \n",
    "If for some reason we wanted to execute on just a subsample of our data, we could do this by selecting only certain samples to call the `step2` function on. Because `step2` is a function of `data`, it will always execute with the parameters that are linked to `data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## example of ways to run step 2 to filter and trim reads\n",
    "#data1.step2(\"1A_0\")            ## run on a single sample\n",
    "data1.step2([\"1B_0\", \"1C_0\"])  ## run on one or more samples\n",
    "#data1.step2()                  ## run on all samples, skipping finished ones\n",
    "\n",
    "## print the results\n",
    "print data1.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the read mapping (SE)\n",
    "Here's an example cmdline run with args explained below:\n",
    "\n",
    "smalt map -f sam -n 8 -l pp -o Arremon.sam zf-ref ../MarTum-fasta/ArremonR1.fa ../MarTum-fasta/ArremonR2.fa\n",
    "\n",
    "* -f sams - you can also output as 'bam' but it requires installing bambamc which is explained in the smalt docs, but which seems annoying, esp cuz samtools will do it for us.\n",
    "* -n sets the number of threads to 8, dramatically increases speed\n",
    "* -l pp tells smalt about the orientation of the paired reads, in this case pp means both reads are on the same strand in the 5' to 3' direction, I think the second read was originally from the second strand and pyrad reverse complemented it.\n",
    "* -o is the outfile\n",
    "* Next is the indexed reference sequence and the files containing reads\n",
    "\n",
    "Other options to look into:\n",
    "* -y minid Filters output alignments by a threshold in the number of exactly\n",
    "matching nucleotides.\n",
    "* -r seed Determines how reads or mate pairs with multiple best mappings are\n",
    "reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data1.paramsdict[\"working_directory\"]\n",
    "data1.dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# This is all test junk, ignore\n",
    "###############################\n",
    "output = \"/tmp/wat\"\n",
    "\n",
    "# Check the input files\n",
    "SMALT_CMD = \"check \"\n",
    "## the read1 demultiplexed reads file\n",
    "fr1 = data1.get_params(1)+\"/fastq/1A_0_R1_.gz\"\n",
    "#data1.smalt = \"/usr/local/bin/smalt\"\n",
    "cmd = data1.smalt + \" \" + SMALT_CMD + \" \" + fr1\n",
    "print cmd\n",
    "subprocess.call(cmd, shell=True,\n",
    "                     stderr=subprocess.STDOUT,\n",
    "                     stdout=subprocess.PIPE)\n",
    "\n",
    "SMALT_CMD = \"map -f sam -n 8 -o \" + output\n",
    "## the read1 demultiplexed reads file\n",
    "\n",
    "## TODO: I recommend using parameter descriptions rather than numbers\n",
    "## in the code so it is more robust to potential reordering of parameters\n",
    "fr1 = data1.get_params('working_directory')+\"/fastq/1A_0_R1_.gz\"\n",
    "\n",
    "cmd = data1.smalt + \" \" + SMALT_CMD + \" \" + REFSEQ + \" \" + fr1\n",
    "print cmd\n",
    "subprocess.call(cmd, shell=True,\n",
    "                     stderr=subprocess.STDOUT,\n",
    "                     stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get mapped and unmapped reads\n",
    "\n",
    "First get some info about our mapping.\n",
    "\n",
    "    samtools flagstat <yoursam>\n",
    "\n",
    "Get only the mapped reads. 0x4 is a bitmask for 'unmapped' reads, -F means get all not this mask. In both cases -b outputs as bam\n",
    "\n",
    "    samtools view -b -F 0x4 <your.sam> > mapped.bam\n",
    "\n",
    "Same as above, but in this case -f means just give me the ones with this flag set.\n",
    "\n",
    "    samtools view -b -f 0x4 <your.sam> > unmapped.bam\n",
    "\n",
    "## \n",
    "\n",
    "samtools sort -T /tmp/wat -O bam test.mapped.bam > test.mapped.sorted.bam\n",
    "samtools bam2fq test.mapped.sorted.bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pysam\n",
    "\n",
    "#This is junk\n",
    "\n",
    "print data1.muscle\n",
    "print data1.vsearch\n",
    "print data1.smalt\n",
    "print data1.samples[\"1B_0\"].files.edits\n",
    "#bam2py(\"\")\n",
    "#pysam.view(\"-b\", \"-S\", \"-o\") #, INDIVIDUALS_WORK_DIR+species+\"/\"+ind+\"-\"+refseq.split(\"/\")[-1]+\".bam\", INDIVIDUALS_WORK_DIR+species+\"/\"+ind+\"-\"+refseq.split(\"/\")[-1]+\".sam\", catch_stdout=False)\n",
    "#pysam.sort( \"-O\", \"bam\", \"-o\", INDIVIDUALS_WORK_DIR+species+\"/\"+ind+\"-\"+refseq.split(\"/\")[-1]+\".bam\", \"-T\", \"tempfile\", INDIVIDUALS_WORK_DIR+species+\"/\"+ind+\"-\"+refseq.split(\"/\")[-1]+\".bam\", catch_stdout=False)\n",
    "#pysam.index( INDIVIDUALS_WORK_DIR+species+\"/\"+ind+\"-\"+refseq.split(\"/\")[-1]+\".bam\", catch_stdout=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: clustering within-samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## run step 3 to cluster reads within samples using vsearch\n",
    "#data1.step3(preview=1) #[\"2H_0\", \"2G_0\"], preview=1)\n",
    "data1.step3([\"1B_0\", \"1C_0\"], preview=1)\n",
    "## print the results\n",
    "print data1.stats\n",
    "print data1.samples[\"1B_0\"].files.fastq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of plotting with _ipyrad_\n",
    "There are a a few simple plotting functions in _ipyrad_ useful for visualizing results. These are in the module `ipyrad.plotting`. Below is an interactive plot for visualizing the distributions of coverages across the 12 samples in the test data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ipyrad as ip\n",
    "import ipyrad.plotting as iplot\n",
    "\n",
    "## reload autosaved data. In case you quit and came back \n",
    "#data1 = ip.load_dataobj(\"test_rad/2014_data.dataobj\")\n",
    "\n",
    "## plot for one or more selected samples\n",
    "iplot.depthplot(data1, [\"1A_0\", \"1B_0\"])\n",
    "\n",
    "## plot for all samples in data1\n",
    "#iplot.depthplot(data1)\n",
    "\n",
    "## save plot as pdf and html\n",
    "iplot.depthplot(data1, outprefix=\"testfig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 4: Joint estimation of heterozygosity and error rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## run step 4\n",
    "data1.step4() #\"2H_0\", \"2G_0\")\n",
    "\n",
    "## print the results\n",
    "print data1.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Consensus base calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## run step 5\n",
    "data1.step5([\"2H_0\"])\n",
    "\n",
    "## print the results\n",
    "print data1.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick parameter explanations are always on-hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ip.get_params_info(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log history \n",
    "A common problem after struggling through an analysis is that you find you've completely forgotten what parameters you used at what point, and when you changed them. The log history time stamps all calls to `set_params()`, as well as calls to `step` methods. It also records copies/branching of data objects.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in data1.log:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Assembly objects\n",
    "Assembly objects can be saved and loaded so that interactive analyses can be started, stopped, and returned to quite easily. The format of these saved files is a serialized 'dill' object used by Python. Individual Sample objects are saved within Assembly objects. These objects to not contain the actual sequence data, but only link to it, and so are not very large. The information contained includes parameters and the log of Assembly objects, and the statistics and state of Sample objects. Assembly objects are autosaved each time an assembly `step` function is called, but you can also create your own checkpoints with the `save` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## save assembly object\n",
    "#ip.save_assembly(\"data1.p\")\n",
    "\n",
    "## load assembly object\n",
    "data2 = ip.load_assembly(\"/tmp/ipyrad-test/test.assembly\")\n",
    "print data2.name\n",
    "#print data.stats\n",
    "for sample in data2.samples:\n",
    "    data2.samples[sample].stats.state = 2#.state\n",
    "print data2.stats\n",
    "#data2.set_params(4, \"/tmp/ipyrad-test/test_edits/\")\n",
    "#print data2\n",
    "print data2.samples[\"1A_0\"].files.clusters\n",
    "#data2.step2(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipyrad import assemble\n",
    "data2.step3([\"1A_0\"], preview=True, force=True)\n",
    "#assemble.cluster_within.derep_and_sort( data1, data1.samples[\"3L_0\"], 0 )\n",
    "data2.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assemble.cluster_within.muscle_align( data1, data1.samples[\"1D_0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working ipyparallel toy for testing\n",
    "Most stuff below here requires this codeblock to be run, to init ipyparallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ipyparallel\n",
    "print( ipyparallel.__version__)\n",
    "from ipyparallel import Client\n",
    "ipyclient = Client()\n",
    "print(ipyclient.ids)\n",
    "dview = ipyclient.load_balanced_view()\n",
    "parallel_result = dview.map_async(lambda x:x**10, range(32))\n",
    "print(parallel_result.get())\n",
    "#print(parallel_result)\n",
    "#res = dview.map_async(print, \"Hello, World\")\n",
    "#print(res)\n",
    "del dview\n",
    "ipyclient.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy code for testing cluster_within on a subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ipyclient = Client()\n",
    "print(ipyclient.ids)\n",
    "samp1=\"1A_0\"\n",
    "subsamples = []\n",
    "data1.samples[samp1].stats.state = 2\n",
    "subsamples.append((sample, data1.samples[samp1]))\n",
    "\n",
    "print data1.samples[samp1].files.edits\n",
    "assemble.cluster_within.run( data1, subsamples, ipyclient, True, True, True)\n",
    "ipyclient.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for directly debugging mapreads() outside of ipyparallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Debugging smalt code in cluster_within\n",
    "samp1=\"1A_0\"\n",
    "sample_obj=data1.samples[samp1]\n",
    "data1.samples[samp1].stats.state = 2\n",
    "sample = subsamples[0][1]\n",
    "print sample\n",
    "assemble.cluster_within.mapreads([data2, sample_obj, 1, 0, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Debug derep_and_sort\n",
    "samp1=\"1A_0\"\n",
    "sample_obj=data2.samples[samp1]\n",
    "data2.samples[samp1].stats.state = 2\n",
    "assemble.cluster_within.derep_and_sort(data2, sample_obj, 1, 4)\n",
    "print(sample_obj.files.edits)\n",
    "handle = sample_obj.files.edits[0]\n",
    "cmd = data2.vsearch+\\\n",
    "    \" -derep_fulllength \"+handle+\\\n",
    "    \" \"+\\\n",
    "    \" -output \"+os.path.join(data2.dirs.edits, sample_obj.name+\".derep\")+\\\n",
    "    \" -sizeout \"+\\\n",
    "    \" -threads \"+str(4)+\\\n",
    "    \" -fasta_width 0\"\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "samp1=\"1A_0\"\n",
    "sample=data2.samples[samp1]\n",
    "unmapped_fastq_handle=os.path.join(data2.dirs.edits, sample.name+\".fasta\")\n",
    "with open(os.path.realpath(unmapped_fastq_handle), 'rb') as fq:\n",
    "    quart1 = itertools.izip(*[iter(fq)]*4)\n",
    "    quarts = itertools.izip(quart1, iter(int, 1))\n",
    "    read1 = [i.strip() for i in quart[0]]\n",
    "    writing = []\n",
    "    while 1:\n",
    "        try:\n",
    "            quart = quarts.next()\n",
    "        except StopIteration:\n",
    "            break\n",
    "        read1 = [i.strip() for i in quart[0]]\n",
    "        sseq = \">\"+sample.name+\"_\"+str(0)+\\\n",
    "                           \"_c1\\n\"+read1[1]+\"\\n\"\n",
    "        writing.append(sseq)\n",
    "print(sample.files.edits[0])\n",
    "with open( sample.files.edits[0], 'w' ) as out:\n",
    "    out.write(\"\".join(writing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp1=\"3L_0\"\n",
    "sample=data2.samples[samp1]\n",
    "unmapped_fastq_handle=os.path.join(data2.dirs.edits, sample.name+\".fastq\")\n",
    "print(unmapped_fastq_handle)\n",
    "if True:\n",
    "    writing = []\n",
    "    with open(os.path.realpath(unmapped_fastq_handle), 'rb') as fq:\n",
    "        quart1 = itertools.izip(*[iter(fq)]*4)\n",
    "        quarts = itertools.izip(quart1, iter(int, 1))\n",
    "        writing = []\n",
    "        while 1:\n",
    "            try:\n",
    "                quart = quarts.next()\n",
    "            except StopIteration:\n",
    "                break\n",
    "            read1 = [i.strip() for i in quart[0]]\n",
    "            sseq = \">\"+sample.name+\"_\"+str(0)+\\\n",
    "                           \"_c1\\n\"+read1[1]+\"\\n\"\n",
    "            writing.append(sseq)\n",
    "\n",
    "    with open( sample.files.edits[0], 'w' ) as out:\n",
    "        out.write(\"\".join(writing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quart=[]\n",
    "quarts=[]\n",
    "quart1=[]\n",
    "nthreads=4\n",
    "preview=True\n",
    "samp1=\"1B_0\"\n",
    "sample=data1.samples[samp1]\n",
    "data=data1\n",
    "if True:\n",
    "    samhandle = os.path.join(data.dirs.edits, sample.name+\".sam\")\n",
    "    bamhandle = os.path.join(data.dirs.edits, sample.name+\".bam\")\n",
    "    unmapped_fastq_handle = os.path.join(data.dirs.edits, sample.name+\".fastq\")\n",
    "\n",
    "    ## get call string\n",
    "    cmd = data.smalt+\\\n",
    "        \" map -f sam -n \" + str(nthreads) +\\\n",
    "        \" -o \" + samhandle +\\\n",
    "        \" \" + data.get_params(28) +\\\n",
    "        \" \" + sample.files.fastq[0]\n",
    "\n",
    "    ## run smalt\n",
    "    if preview:\n",
    "        ## make this some kind of wait command that kills after a few mins\n",
    "        subprocess.call(cmd, shell=True,\n",
    "                             stderr=subprocess.STDOUT,\n",
    "                             stdout=subprocess.PIPE)\n",
    "    else:\n",
    "        subprocess.call(cmd, shell=True,\n",
    "                             stderr=subprocess.STDOUT,\n",
    "                             stdout=subprocess.PIPE)\n",
    "\n",
    "    cmd = data.samtools+\\\n",
    "        \" view -b -f 0x4 \"+samhandle+\\\n",
    "            \" > \" + bamhandle\n",
    "    subprocess.call(cmd, shell=True,\n",
    "                         stderr=subprocess.STDOUT,\n",
    "                         stdout=subprocess.PIPE)\n",
    "\n",
    "    cmd = data.samtools+\\\n",
    "        \" sort -T \"+samhandle+\".tmp\" +\\\n",
    "        \" -O bam \"+bamhandle+\\\n",
    "        \" -o \"+bamhandle+\".sorted\"\n",
    "    subprocess.call(cmd, shell=True,\n",
    "                         stderr=subprocess.STDOUT,\n",
    "                         stdout=subprocess.PIPE)\n",
    "    cmd = data.samtools+\\\n",
    "        \" bam2fq \"+bamhandle+\".sorted\"+\\\n",
    "        \" > \"+unmapped_fastq_handle\n",
    "    subprocess.call(cmd, shell=True,\n",
    "                         stderr=subprocess.STDOUT,\n",
    "                         stdout=subprocess.PIPE)\n",
    "\n",
    "    ## This is hax to get fastq to fasta to get this off the ground.\n",
    "    ## samtools bam2fq natively returns fastq, you just delete this code\n",
    "    ## when fastq pipleline is working\n",
    "    writing = []\n",
    "    with open(os.path.realpath(unmapped_fastq_handle), 'rb') as fq:\n",
    "        quart1 = itertools.izip(*[iter(fq)]*4)\n",
    "        quarts = itertools.izip(quart1, iter(int, 1))\n",
    "        writing = []\n",
    "        while 1:\n",
    "            try:\n",
    "                quart = quarts.next()\n",
    "            except StopIteration:\n",
    "                break\n",
    "            read1 = [i.strip() for i in quart[0]]\n",
    "            sseq = \">\"+sample.name+\"_\"+str(0)+\\\n",
    "                           \"_c1\\n\"+read1[1]+\"\\n\"\n",
    "            writing.append(sseq)\n",
    "\n",
    "    with open( sample.files.edits[0], 'w' ) as out:\n",
    "        out.write(\"\".join(writing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd = data.samtools+\\\n",
    "    \" flagstat \"+bamhandle+\".sorted\"\n",
    "result = subprocess.check_output(cmd, shell=True, \n",
    "                            stderr=subprocess.STDOUT )\n",
    "print(result)\n",
    "\n",
    "#result = subprocess.check_output( cmd, shell=True, \n",
    "#                                       stderr=subprocess.STDOUT )\n",
    "sample.stats.refseq_unmapped_reads=int(result.split()[0])\n",
    "print(sample.stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ipyrad as ip\n",
    "TEST = ip.load_assembly(\"/tmp/ipyrad-test/test-refseq.assembly\")\n",
    "TEST.get_params()\n",
    "TEST.step3( [\"1A_0\"], preview=True, force=True)\n",
    "TEST.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hax for testing why clustering unmapped reads returns so few clusters???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip \n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "sample = TEST.samples[\"1A_0\"]\n",
    "data = TEST\n",
    "#ip.assemble.cluster_within.cleanup(TEST, TEST.samples[\"1A_0\"])\n",
    "if True:\n",
    "    sample.files.clusters = os.path.join(data.dirs.clusts,\n",
    "                                         sample.name+\".clustS.gz\")\n",
    "\n",
    "    print(sample.files.clusters)\n",
    "    ## get depth stats\n",
    "    infile = gzip.open(sample.files.clusters)\n",
    "    duo = itertools.izip(*[iter(infile)]*2)\n",
    "    depth = []\n",
    "    thisdepth = []\n",
    "    while 1:\n",
    "        try:\n",
    "            itera = duo.next()[0]\n",
    "            #print(itera)\n",
    "        except StopIteration:\n",
    "            print(\"wat\")\n",
    "            break\n",
    "        if itera != \"//\\n\":\n",
    "            thisdepth.append(int(itera.split(\";\")[-2][5:]))\n",
    "        else:\n",
    "            ## append and reset\n",
    "            depth.append(sum(thisdepth))\n",
    "            thisdepth = []\n",
    "    infile.close()\n",
    "\n",
    "    if depth:\n",
    "        ## make sense of stats\n",
    "        depth = np.array(depth)\n",
    "        print(depth)\n",
    "        keepmj = depth[depth >= data.paramsdict[\"mindepth_majrule\"]]\n",
    "        keepstat = depth[depth >= data.paramsdict[\"mindepth_statistical\"]]\n",
    "        ## sample assignments\n",
    "        sample.stats[\"state\"] = 3\n",
    "        sample.stats[\"clusters_total\"] = len(depth)\n",
    "        sample.stats[\"clusters_kept\"] = max([len(i) for i in \\\n",
    "                                             (keepmj, keepstat)])\n",
    "        sample.depths.total = depth\n",
    "        sample.depths.mjmin = keepmj\n",
    "        sample.depths.statmin = keepstat\n",
    "\n",
    "        data.stamp(\"s3 clustering on \"+sample.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
