{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _ipyrad_ testing tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "Import _ipyrad_ and remove previous test files if they are already present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.65\n"
     ]
    }
   ],
   "source": [
    "## import modules\n",
    "import ipyrad as ip      ## for RADseq assembly\n",
    "print ip.__version__     ## print version\n",
    "\n",
    "## clear data from test directory if it already exists\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "#if os.path.exists(\"./test_refseq/\"):\n",
    "#    shutil.rmtree(\"./test_refseq/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This is useful during development since IPython\n",
    "## seems to want to re-use old .pyc files, \n",
    "## though even this doesn't always work...\n",
    "import IPython.lib.deepreload\n",
    "import __builtin__\n",
    "from IPython.lib import deepreload\n",
    "__builtin__.reload = deepreload.reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize smalt (index reference sequence)\n",
    "This is preparation for indexing. It only ever needs to be done once so shoud be tested during initialization.\n",
    "\n",
    "`smalt index zf-ref ../zf/zf.sm.fa`\n",
    "\n",
    "There is an optional -s flag that could improve mapping accuracy. Consider the best default, probably not worth letting people pass it in, if they want to mess with it they can index their own reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1A_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-eee2dfa41c3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# TODO: create and link index files to Sample objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'1A_0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_smi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'...'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'1A_0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_sma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'...'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '1A_0'"
     ]
    }
   ],
   "source": [
    "# hack the binary paths cuz the current egg doesn't have them in it\n",
    "#data1.muscle=\n",
    "#data1.vsearch\n",
    "#data1.smalt\n",
    "\n",
    "# Reference sequence directory (gzipped fasta files)\n",
    "# TODO: set this as a parameter\n",
    "# e.g., data1.set_params('refseq', \"./data/zf.fa.gz\")\n",
    "\n",
    "# TODO: push this example file to the data/ dir \n",
    "REFSEQ = \"./data/zf.fa.gz\"\n",
    "\n",
    "# Set the step size to 4 (default is 13)\n",
    "# This will slow down read mapping, but increase accuracy\n",
    "SMALT_INDEX_FLAGS = \" -s 4 \"\n",
    "\n",
    "# TODO: create and link a dir/ to the Assembly object for the reference data files\n",
    "data1.dirs.reference = '...'\n",
    "\n",
    "# TODO: create and link index files to Sample objects\n",
    "data1.samples['1A_0'].files.index_smi = '...'\n",
    "data1.samples['1A_0'].files.index_sma = '...'\n",
    "\n",
    "# Test if reference sequence is already indexed\n",
    "# Only index if the .smi and .sma files don't exist, saves lots of time\n",
    "if not os.path.isfile( REFSEQ+\".smi\" ):\n",
    "    # smalt indexing will create two files called REFSEQ.smi and .sma\n",
    "    # in the same directory as the reference sequence. \n",
    "    cmd = data1.smalt + \" index \" + SMALT_INDEX_FLAGS + REFSEQ + \" \" + REFSEQ\n",
    "    print cmd\n",
    "    subprocess.check_call(cmd, shell=True,\n",
    "                            stderr=subprocess.STDOUT,\n",
    "                            stdout=subprocess.PIPE)\n",
    "    #output = subprocess.check_output( \" \".join(cmd), shell=True)\n",
    "else:\n",
    "    print \"Reference sequence index exists\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembly and Sample objects\n",
    "\n",
    "Assembly and Sample objects are used by _ipyrad_ to access data stored on disk and to manipulate it. Each biological sample in a data set is represented in a Sample object, and these Samples are stored inside Assembly objects. The Assembly object contains functions to assemble the data, and stores a log of all steps performed and the resulting statistics of those steps. Assembly objects can be copied or merged to allow branching events where different parameters are applied to assemblies. \n",
    "\n",
    "To create an Assembly object call ip.Assembly and pass it a name for the data set. We could imagine that we planned to assemble and later combine data from multiple sequencing runs, but before combining them each group of samples has to be analyzed under a different set of parameters. As an example, we could call two data sets \"2014_data\" and \"2015_data\". These initially do not contain any Samples. Sample objects are created either by linking fastq files to the Assembly object or by running step 1 to demultiplex raw data files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0 new Samples created in 2014_data.\n",
      "0 fastq files linked to Samples.\n",
      "[]\n",
      "0 new Samples created in 2015_data.\n",
      "0 fastq files linked to Samples.\n",
      "Assembly object named 2014_data\n",
      "Assembly object named 2015_data\n"
     ]
    }
   ],
   "source": [
    "## create an Assembly object called data1. \n",
    "## It takes an 'test'\n",
    "data1 = ip.Assembly(\"2014_data\")\n",
    "data2 = ip.Assembly(\"2015_data\")\n",
    "\n",
    "print \"Assembly object named\", data1.name\n",
    "print \"Assembly object named\", data2.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying assembly parameters\n",
    "All of the parameter settings are linked to an Assembly object, which has a set of default parameters when it is created. These can be viewed using the `get_params()` function. To get more detailed information about all paramteres use `ip.get_params_info()` or to select a single parameter use `ip.get_params_info(3)`. Assembly objects have a function `set_params()` that can be used to modify parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0 new Samples created in 2014_data.\n",
      "0 fastq files linked to Samples.\n",
      "  1   working_directory             ./test_refseq                                \n",
      "  2   raw_fastq_path                ./data/sim_rad_test_R1_.fastq.gz             \n",
      "  3   barcodes_path                 ./data/sim_rad_test_barcodes.txt             \n",
      "  4   sorted_fastq_path             ./test/test_refseq/2014_data_fastqs/         \n",
      "  5   restriction_overhang          ('TGCAG', '')                                \n",
      "  6   max_low_qual_bases            5                                            \n",
      "  7   N_processors                  3                                            \n",
      "  8   mindepth_statistical          6                                            \n",
      "  9   mindepth_majrule              6                                            \n",
      "  10  datatype                      rad                                          \n",
      "  11  clust_threshold               0.85                                         \n",
      "  12  minsamp                       4                                            \n",
      "  13  max_shared_heterozygosity     0.25                                         \n",
      "  14  prefix_outname                2014_data                                    \n",
      "  15  phred_Qscore_offset           33                                           \n",
      "  16  max_barcode_mismatch          1                                            \n",
      "  17  filter_adapters               0                                            \n",
      "  18  filter_min_trim_len           35                                           \n",
      "  19  ploidy                        2                                            \n",
      "  20  max_stack_size                1000                                         \n",
      "  21  max_Ns_consens                5                                            \n",
      "  22  max_Hs_consens                8                                            \n",
      "  23  max_SNPs_locus                (100, 100)                                   \n",
      "  24  max_Indels_locus              (5, 99)                                      \n",
      "  25  trim_overhang                 (1, 2, 2, 1)                                 \n",
      "  26  hierarchical_clustering       0                                            \n",
      "  27  reference_sequence            /Volumes/WorkDrive/ipyrad/refhacking/MusChr1.fa\n"
     ]
    }
   ],
   "source": [
    "## modify parameters for this Assembly object\n",
    "data1.set_params(1, \"./test_refseq\")\n",
    "data1.set_params(2, \"./data/sim_rad_test_R1_.fastq.gz\")\n",
    "data1.set_params(3, \"./data/sim_rad_test_barcodes.txt\")\n",
    "data1.set_params(4, \"./test/test_refseq/2014_data_fastqs/\")\n",
    "data1.set_params(7, 3)\n",
    "data1.set_params(10, 'rad')\n",
    "data1.set_params(28, '/Volumes/WorkDrive/ipyrad/refhacking/MusChr1.fa')\n",
    "\n",
    "## print the new parameters to screen\n",
    "data1.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting data assembly and Sample objects\n",
    "If the data are already demultiplexed then fastq files can be linked directly to the Data object, which in turn will create Sample objects for each fastq file (or pair of fastq files for paired data). The files may be gzip compressed. If the data are not demultiplexed then you will have to run the step1 function below to demultiplex the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0 new Samples created in 2014_data.\n",
      "0 fastq files linked to Samples.\n"
     ]
    }
   ],
   "source": [
    "## This would link fastq files from the 'sorted_fastq_path' if present\n",
    "## Here it does nothing b/c there are no files in the sorted_fastq_path\n",
    "data1.link_fastqs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Demultiplex the raw data files\n",
    "This uses the barcodes information to demultiplex reads in data files found in the 'raw_fastq_path'. It will create a Sample object for each sample that will be stored in the Assembly object. The state of each sample will be set to 1, meaning that the sample has completed step 1 of the _ipyrad_ assembly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples already found in 2014_data use ip.merge() to combine samples \n",
      "from multipleAssembly objects\n",
      "      state  reads_raw\n",
      "1A_0      1      20099\n",
      "1B_0      1      19977\n",
      "1C_0      1      20114\n",
      "1D_0      1      19895\n",
      "2E_0      1      19928\n",
      "2F_0      1      19934\n",
      "2G_0      1      20026\n",
      "2H_0      1      19936\n",
      "3I_0      1      20084\n",
      "3J_0      1      20011\n",
      "3K_0      1      20117\n",
      "3L_0      1      19901\n"
     ]
    }
   ],
   "source": [
    "## run step 1 to demultiplex the data\n",
    "data1.step1(preview=1)\n",
    "\n",
    "## print the results for each Sample in data1\n",
    "print data1.stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Filter reads \n",
    "If for some reason we wanted to execute on just a subsample of our data, we could do this by selecting only certain samples to call the `step2` function on. Because `step2` is a function of `data`, it will always execute with the parameters that are linked to `data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples already found in 2014_data use ip.merge() to combine samples \n",
      "from multipleAssembly objects\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "drop() got an unexpected keyword argument 'how'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0edce312c041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## print the results for each Sample in data1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mwat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/anaconda/lib/python2.7/site-packages/ipyrad-0.0.65-py2.7.egg/ipyrad/core/assembly.pyc\u001b[0m in \u001b[0;36mstats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mnameordered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         return pd.DataFrame([self.samples[i].stats for i in nameordered], \n\u001b[0;32m--> 166\u001b[0;31m                       index=nameordered).drop(axis=1, how='all')\n\u001b[0m\u001b[1;32m    167\u001b[0m                       \u001b[0;31m#dtype=[int, int, int, int, int, float, float, int])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: drop() got an unexpected keyword argument 'how'"
     ]
    }
   ],
   "source": [
    "## run step 1 to demultiplex the data\n",
    "data1.step1()\n",
    "\n",
    "## print the results for each Sample in data1\n",
    "print data1.stats\n",
    "wat = data1.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Filter reads \n",
    "If for some reason we wanted to execute on just a subsample of our data, we could do this by selecting only certain samples to call the `step2` function on. Because `step2` is a function of `data`, it will always execute with the parameters that are linked to `data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      state  reads_raw  reads_filtered  clusters_total  clusters_kept  \\\n",
      "1A_0      1      20099             NaN             NaN            NaN   \n",
      "1B_0      2      19977           19977             NaN            NaN   \n",
      "1C_0      2      20114           20114             NaN            NaN   \n",
      "1D_0      1      19895             NaN             NaN            NaN   \n",
      "2E_0      1      19928             NaN             NaN            NaN   \n",
      "2F_0      1      19934             NaN             NaN            NaN   \n",
      "2G_0      1      20026             NaN             NaN            NaN   \n",
      "2H_0      1      19936             NaN             NaN            NaN   \n",
      "3I_0      1      20084             NaN             NaN            NaN   \n",
      "3J_0      1      20011             NaN             NaN            NaN   \n",
      "3K_0      1      20117             NaN             NaN            NaN   \n",
      "3L_0      1      19901             NaN             NaN            NaN   \n",
      "\n",
      "      hetero_est  error_est  reads_consens  \n",
      "1A_0         NaN        NaN            NaN  \n",
      "1B_0         NaN        NaN            NaN  \n",
      "1C_0         NaN        NaN            NaN  \n",
      "1D_0         NaN        NaN            NaN  \n",
      "2E_0         NaN        NaN            NaN  \n",
      "2F_0         NaN        NaN            NaN  \n",
      "2G_0         NaN        NaN            NaN  \n",
      "2H_0         NaN        NaN            NaN  \n",
      "3I_0         NaN        NaN            NaN  \n",
      "3J_0         NaN        NaN            NaN  \n",
      "3K_0         NaN        NaN            NaN  \n",
      "3L_0         NaN        NaN            NaN  \n"
     ]
    }
   ],
   "source": [
    "## example of ways to run step 2 to filter and trim reads\n",
    "#data1.step2(\"1A_0\")            ## run on a single sample\n",
    "data1.step2([\"1B_0\", \"1C_0\"])  ## run on one or more samples\n",
    "#data1.step2()                  ## run on all samples, skipping finished ones\n",
    "\n",
    "## print the results\n",
    "print data1.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the read mapping (SE)\n",
    "Here's an example cmdline run with args explained below:\n",
    "\n",
    "smalt map -f sam -n 8 -l pp -o Arremon.sam zf-ref ../MarTum-fasta/ArremonR1.fa ../MarTum-fasta/ArremonR2.fa\n",
    "\n",
    "* -f sams - you can also output as 'bam' but it requires installing bambamc which is explained in the smalt docs, but which seems annoying, esp cuz samtools will do it for us.\n",
    "* -n sets the number of threads to 8, dramatically increases speed\n",
    "* -l pp tells smalt about the orientation of the paired reads, in this case pp means both reads are on the same strand in the 5' to 3' direction, I think the second read was originally from the second strand and pyrad reverse complemented it.\n",
    "* -o is the outfile\n",
    "* Next is the indexed reference sequence and the files containing reads\n",
    "\n",
    "Other options to look into:\n",
    "* -y minid Filters output alignments by a threshold in the number of exactly\n",
    "matching nucleotides.\n",
    "* -r seed Determines how reads or mate pairs with multiple best mappings are\n",
    "reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data1.paramsdict[\"working_directory\"]\n",
    "data1.dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# This is all test junk, ignore\n",
    "###############################\n",
    "output = \"/tmp/wat\"\n",
    "\n",
    "# Check the input files\n",
    "SMALT_CMD = \"check \"\n",
    "## the read1 demultiplexed reads file\n",
    "fr1 = data1.get_params(1)+\"/fastq/1A_0_R1_.gz\"\n",
    "#data1.smalt = \"/usr/local/bin/smalt\"\n",
    "cmd = data1.smalt + \" \" + SMALT_CMD + \" \" + fr1\n",
    "print cmd\n",
    "subprocess.call(cmd, shell=True,\n",
    "                     stderr=subprocess.STDOUT,\n",
    "                     stdout=subprocess.PIPE)\n",
    "\n",
    "SMALT_CMD = \"map -f sam -n 8 -o \" + output\n",
    "## the read1 demultiplexed reads file\n",
    "\n",
    "## TODO: I recommend using parameter descriptions rather than numbers\n",
    "## in the code so it is more robust to potential reordering of parameters\n",
    "fr1 = data1.get_params('working_directory')+\"/fastq/1A_0_R1_.gz\"\n",
    "\n",
    "cmd = data1.smalt + \" \" + SMALT_CMD + \" \" + REFSEQ + \" \" + fr1\n",
    "print cmd\n",
    "subprocess.call(cmd, shell=True,\n",
    "                     stderr=subprocess.STDOUT,\n",
    "                     stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get mapped and unmapped reads\n",
    "\n",
    "First get some info about our mapping.\n",
    "\n",
    "    samtools flagstat <yoursam>\n",
    "\n",
    "Get only the mapped reads. 0x4 is a bitmask for 'unmapped' reads, -F means get all not this mask. In both cases -b outputs as bam\n",
    "\n",
    "    samtools view -b -F 0x4 <your.sam> > mapped.bam\n",
    "\n",
    "Same as above, but in this case -f means just give me the ones with this flag set.\n",
    "\n",
    "    samtools view -b -f 0x4 <your.sam> > unmapped.bam\n",
    "\n",
    "## \n",
    "\n",
    "samtools sort -T /tmp/wat -O bam test.mapped.bam > test.mapped.sorted.bam\n",
    "samtools bam2fq test.mapped.sorted.bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deren/Dropbox/ipyrad/bin/muscle3.8.31_i86linux64\n",
      "/home/deren/Dropbox/ipyrad/bin/vsearch-1.1.3-linux-x86_64\n",
      "/home/deren/Dropbox/ipyrad/bin/smalt-0.7.6-linux-x86_64\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pysam\n",
    "\n",
    "#This is junk\n",
    "\n",
    "print data1.muscle\n",
    "print data1.vsearch\n",
    "print data1.smalt\n",
    "print data1.samples[\"1B_0\"].files.edits\n",
    "#bam2py(\"\")\n",
    "#pysam.view(\"-b\", \"-S\", \"-o\") #, INDIVIDUALS_WORK_DIR+species+\"/\"+ind+\"-\"+refseq.split(\"/\")[-1]+\".bam\", INDIVIDUALS_WORK_DIR+species+\"/\"+ind+\"-\"+refseq.split(\"/\")[-1]+\".sam\", catch_stdout=False)\n",
    "#pysam.sort( \"-O\", \"bam\", \"-o\", INDIVIDUALS_WORK_DIR+species+\"/\"+ind+\"-\"+refseq.split(\"/\")[-1]+\".bam\", \"-T\", \"tempfile\", INDIVIDUALS_WORK_DIR+species+\"/\"+ind+\"-\"+refseq.split(\"/\")[-1]+\".bam\", catch_stdout=False)\n",
    "#pysam.index( INDIVIDUALS_WORK_DIR+species+\"/\"+ind+\"-\"+refseq.split(\"/\")[-1]+\".bam\", catch_stdout=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: clustering within-samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for reference sequence index. If it doesn't exist then create it.\n",
      "This could take several minutes, but it's a one time penalty, so be patient.\n",
      "None\n",
      "Clustering 2 samples on 3 processors.\n",
      ".\n",
      "      state  reads_raw  reads_filtered  clusters_total  clusters_kept  \\\n",
      "1A_0      1      20099             NaN             NaN            NaN   \n",
      "1B_0      3      19977           19977               1              0   \n",
      "1C_0      3      20114           20114               1              0   \n",
      "1D_0      1      19895             NaN             NaN            NaN   \n",
      "2E_0      1      19928             NaN             NaN            NaN   \n",
      "2F_0      1      19934             NaN             NaN            NaN   \n",
      "2G_0      1      20026             NaN             NaN            NaN   \n",
      "2H_0      1      19936             NaN             NaN            NaN   \n",
      "3I_0      1      20084             NaN             NaN            NaN   \n",
      "3J_0      1      20011             NaN             NaN            NaN   \n",
      "3K_0      1      20117             NaN             NaN            NaN   \n",
      "3L_0      1      19901             NaN             NaN            NaN   \n",
      "\n",
      "      hetero_est  error_est  reads_consens  \n",
      "1A_0         NaN        NaN            NaN  \n",
      "1B_0         NaN        NaN            NaN  \n",
      "1C_0         NaN        NaN            NaN  \n",
      "1D_0         NaN        NaN            NaN  \n",
      "2E_0         NaN        NaN            NaN  \n",
      "2F_0         NaN        NaN            NaN  \n",
      "2G_0         NaN        NaN            NaN  \n",
      "2H_0         NaN        NaN            NaN  \n",
      "3I_0         NaN        NaN            NaN  \n",
      "3J_0         NaN        NaN            NaN  \n",
      "3K_0         NaN        NaN            NaN  \n",
      "3L_0         NaN        NaN            NaN  \n",
      "/Volumes/WorkDrive/ipyrad/ipyrad/tests/test_refseq/2014_data_fastqs/1B_0_R1_.gz\n"
     ]
    }
   ],
   "source": [
    "## run step 3 to cluster reads within samples using vsearch\n",
    "#data1.step3(preview=1) #[\"2H_0\", \"2G_0\"], preview=1)\n",
    "data1.step3([\"1B_0\", \"1C_0\"], preview=1)\n",
    "## print the results\n",
    "print data1.stats\n",
    "print data1.samples[\"1B_0\"].files.fastq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of plotting with _ipyrad_\n",
    "There are a a few simple plotting functions in _ipyrad_ useful for visualizing results. These are in the module `ipyrad.plotting`. Below is an interactive plot for visualizing the distributions of coverages across the 12 samples in the test data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ipyrad as ip\n",
    "import ipyrad.plotting as iplot\n",
    "\n",
    "## reload autosaved data. In case you quit and came back \n",
    "#data1 = ip.load_dataobj(\"test_rad/2014_data.dataobj\")\n",
    "\n",
    "## plot for one or more selected samples\n",
    "iplot.depthplot(data1, [\"1A_0\", \"1B_0\"])\n",
    "\n",
    "## plot for all samples in data1\n",
    "#iplot.depthplot(data1)\n",
    "\n",
    "## save plot as pdf and html\n",
    "iplot.depthplot(data1, outprefix=\"testfig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 4: Joint estimation of heterozygosity and error rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## run step 4\n",
    "data1.step4() #\"2H_0\", \"2G_0\")\n",
    "\n",
    "## print the results\n",
    "print data1.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Consensus base calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## run step 5\n",
    "data1.step5([\"2H_0\"])\n",
    "\n",
    "## print the results\n",
    "print data1.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick parameter explanations are always on-hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ip.get_params_info(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log history \n",
    "A common problem after struggling through an analysis is that you find you've completely forgotten what parameters you used at what point, and when you changed them. The log history time stamps all calls to `set_params()`, as well as calls to `step` methods. It also records copies/branching of data objects.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in data1.log:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Assembly objects\n",
    "Assembly objects can be saved and loaded so that interactive analyses can be started, stopped, and returned to quite easily. The format of these saved files is a serialized 'dill' object used by Python. Individual Sample objects are saved within Assembly objects. These objects to not contain the actual sequence data, but only link to it, and so are not very large. The information contained includes parameters and the log of Assembly objects, and the statistics and state of Sample objects. Assembly objects are autosaved each time an assembly `step` function is called, but you can also create your own checkpoints with the `save` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "      state  reads_raw  reads_filtered  clusters_total  clusters_kept  \\\n",
      "1A_0      2      20099           20099               1              0   \n",
      "1B_0      2      19977           19977               1              0   \n",
      "1C_0      2      20114           20114               1              0   \n",
      "1D_0      2      19895           19895               1              0   \n",
      "2E_0      2      19928           19928               1              0   \n",
      "2F_0      2      19934           19934               1              0   \n",
      "2G_0      2      20026           20026               1              0   \n",
      "2H_0      2      19936           19936               1              0   \n",
      "3I_0      2      20084           20084               1              0   \n",
      "3J_0      2      20011           20011               1              0   \n",
      "3K_0      2      20117           20117               1              0   \n",
      "3L_0      2      19901           19901               1              0   \n",
      "\n",
      "      hetero_est  error_est  reads_consens  \n",
      "1A_0         NaN        NaN            NaN  \n",
      "1B_0         NaN        NaN            NaN  \n",
      "1C_0         NaN        NaN            NaN  \n",
      "1D_0         NaN        NaN            NaN  \n",
      "2E_0         NaN        NaN            NaN  \n",
      "2F_0         NaN        NaN            NaN  \n",
      "2G_0         NaN        NaN            NaN  \n",
      "2H_0         NaN        NaN            NaN  \n",
      "3I_0         NaN        NaN            NaN  \n",
      "3J_0         NaN        NaN            NaN  \n",
      "3K_0         NaN        NaN            NaN  \n",
      "3L_0         NaN        NaN            NaN  \n",
      "/private/tmp/ipyrad-test/test_clust_0.85/1A_0.clustS.gz\n"
     ]
    }
   ],
   "source": [
    "## save assembly object\n",
    "#ip.save_assembly(\"data1.p\")\n",
    "\n",
    "## load assembly object\n",
    "data2 = ip.load_assembly(\"/tmp/ipyrad-test/test.assembly\")\n",
    "print data2.name\n",
    "#print data.stats\n",
    "for sample in data2.samples:\n",
    "    data2.samples[sample].stats.state = 2#.state\n",
    "print data2.stats\n",
    "#data2.set_params(4, \"/tmp/ipyrad-test/test_edits/\")\n",
    "#print data2\n",
    "print data2.samples[\"1A_0\"].files.clusters\n",
    "#data2.step2(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for reference sequence index. If it doesn't exist then create it.\n",
      "This could take several minutes, but it's a one time penalty, so be patient.\n",
      "None\n",
      "Clustering 1 samples on 4 processors.\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/anaconda/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>reads_raw</th>\n",
       "      <th>reads_filtered</th>\n",
       "      <th>clusters_total</th>\n",
       "      <th>clusters_kept</th>\n",
       "      <th>hetero_est</th>\n",
       "      <th>error_est</th>\n",
       "      <th>reads_consens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1A_0</th>\n",
       "      <td>3</td>\n",
       "      <td>20099</td>\n",
       "      <td>20099</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1B_0</th>\n",
       "      <td>2</td>\n",
       "      <td>19977</td>\n",
       "      <td>19977</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1C_0</th>\n",
       "      <td>2</td>\n",
       "      <td>20114</td>\n",
       "      <td>20114</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1D_0</th>\n",
       "      <td>2</td>\n",
       "      <td>19895</td>\n",
       "      <td>19895</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2E_0</th>\n",
       "      <td>2</td>\n",
       "      <td>19928</td>\n",
       "      <td>19928</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2F_0</th>\n",
       "      <td>2</td>\n",
       "      <td>19934</td>\n",
       "      <td>19934</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2G_0</th>\n",
       "      <td>2</td>\n",
       "      <td>20026</td>\n",
       "      <td>20026</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2H_0</th>\n",
       "      <td>2</td>\n",
       "      <td>19936</td>\n",
       "      <td>19936</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3I_0</th>\n",
       "      <td>2</td>\n",
       "      <td>20084</td>\n",
       "      <td>20084</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3J_0</th>\n",
       "      <td>2</td>\n",
       "      <td>20011</td>\n",
       "      <td>20011</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3K_0</th>\n",
       "      <td>2</td>\n",
       "      <td>20117</td>\n",
       "      <td>20117</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3L_0</th>\n",
       "      <td>2</td>\n",
       "      <td>19901</td>\n",
       "      <td>19901</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      state  reads_raw  reads_filtered  clusters_total  clusters_kept  \\\n",
       "1A_0      3      20099           20099               1              0   \n",
       "1B_0      2      19977           19977               1              0   \n",
       "1C_0      2      20114           20114               1              0   \n",
       "1D_0      2      19895           19895               1              0   \n",
       "2E_0      2      19928           19928               1              0   \n",
       "2F_0      2      19934           19934               1              0   \n",
       "2G_0      2      20026           20026               1              0   \n",
       "2H_0      2      19936           19936               1              0   \n",
       "3I_0      2      20084           20084               1              0   \n",
       "3J_0      2      20011           20011               1              0   \n",
       "3K_0      2      20117           20117               1              0   \n",
       "3L_0      2      19901           19901               1              0   \n",
       "\n",
       "      hetero_est  error_est  reads_consens  \n",
       "1A_0         NaN        NaN            NaN  \n",
       "1B_0         NaN        NaN            NaN  \n",
       "1C_0         NaN        NaN            NaN  \n",
       "1D_0         NaN        NaN            NaN  \n",
       "2E_0         NaN        NaN            NaN  \n",
       "2F_0         NaN        NaN            NaN  \n",
       "2G_0         NaN        NaN            NaN  \n",
       "2H_0         NaN        NaN            NaN  \n",
       "3I_0         NaN        NaN            NaN  \n",
       "3J_0         NaN        NaN            NaN  \n",
       "3K_0         NaN        NaN            NaN  \n",
       "3L_0         NaN        NaN            NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipyrad import assemble\n",
    "data2.step3([\"1A_0\"], preview=True, force=True)\n",
    "#assemble.cluster_within.derep_and_sort( data1, data1.samples[\"3L_0\"], 0 )\n",
    "data2.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assemble.cluster_within.muscle_align( data1, data1.samples[\"1D_0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working ipyparallel toy for testing\n",
    "Most stuff below here requires this codeblock to be run, to init ipyparallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.0\n",
      "[0, 1, 2, 3]\n",
      "[0, 1, 1024, 59049, 1048576, 9765625, 60466176, 282475249, 1073741824, 3486784401, 10000000000, 25937424601, 61917364224, 137858491849, 289254654976, 576650390625, 1099511627776, 2015993900449, 3570467226624, 6131066257801, 10240000000000, 16679880978201, 26559922791424, 41426511213649, 63403380965376, 95367431640625, 141167095653376, 205891132094649, 296196766695424, 420707233300201, 590490000000000, 819628286980801]\n"
     ]
    }
   ],
   "source": [
    "import ipyparallel\n",
    "print( ipyparallel.__version__)\n",
    "from ipyparallel import Client\n",
    "ipyclient = Client()\n",
    "print(ipyclient.ids)\n",
    "dview = ipyclient.load_balanced_view()\n",
    "parallel_result = dview.map_async(lambda x:x**10, range(32))\n",
    "print(parallel_result.get())\n",
    "#print(parallel_result)\n",
    "#res = dview.map_async(print, \"Hello, World\")\n",
    "#print(res)\n",
    "del dview\n",
    "ipyclient.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy code for testing cluster_within on a subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "[]\n"
     ]
    },
    {
     "ename": "CompositeError",
     "evalue": "one or more exceptions from call to method: mapreads\n[3:apply]: IndexError: list index out of range",
     "output_type": "error",
     "traceback": [
      "[3:apply]: ",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m",
      "\u001b[0;32m/Volumes/WorkDrive/ipyrad/tmp/ipyrad/ipyrad/assemble/cluster_within.py\u001b[0m in \u001b[0;36mmapreads\u001b[0;34m(args)\u001b[0m",
      "\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[1;32m    603\u001b[0m     \u001b[0;31m## get args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0;32m--> 604\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoreverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthreads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[1;32m    606\u001b[0m     \u001b[0;31m## preview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      ""
     ]
    }
   ],
   "source": [
    "ipyclient = Client()\n",
    "print(ipyclient.ids)\n",
    "samp1=\"1A_0\"\n",
    "subsamples = []\n",
    "data1.samples[samp1].stats.state = 2\n",
    "subsamples.append((sample, data1.samples[samp1]))\n",
    "\n",
    "print data1.samples[samp1].files.edits\n",
    "assemble.cluster_within.run( data1, subsamples, ipyclient, True, True, True)\n",
    "ipyclient.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for directly debugging mapreads() outside of ipyparallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipyrad.core.sample.Sample object at 0x112d8c190>\n",
      "preview: in run_full, using 4\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "No such attribute: edits",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-78f7148e44cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubsamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0massemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_within\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapreads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/anaconda/lib/python2.7/site-packages/ipyrad-0.0.65-py2.7.egg/ipyrad/assemble/cluster_within.pyc\u001b[0m in \u001b[0;36mmapreads\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;31m## It would be ideal to read in the fastq instead, since this contains info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;31m## about base quality scores, improves the mapping confidence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m     \u001b[0;31m## get args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoreverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthreads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/anaconda/lib/python2.7/site-packages/ipyrad-0.0.65-py2.7.egg/ipyrad/assemble/worker.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such attribute: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: No such attribute: edits"
     ]
    }
   ],
   "source": [
    "#Debugging smalt code in cluster_within\n",
    "samp1=\"1A_0\"\n",
    "sample_obj=data1.samples[samp1]\n",
    "data1.samples[samp1].stats.state = 2\n",
    "sample = subsamples[0][1]\n",
    "print sample\n",
    "assemble.cluster_within.mapreads([data2, sample_obj, 1, 0, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/private/tmp/ipyrad-test/test_edits/1A_0.fasta']\n",
      "/usr/local/opt/anaconda/lib/python2.7/site-packages/ipyrad-0.0.65-py2.7.egg/bin/vsearch-1.1.3-osx-x86_64 -derep_fulllength /private/tmp/ipyrad-test/test_edits/1A_0.fasta  -output /private/tmp/ipyrad-test/test_edits/1A_0.derep -sizeout  -threads 4 -fasta_width 0\n"
     ]
    }
   ],
   "source": [
    "# Debug derep_and_sort\n",
    "samp1=\"1A_0\"\n",
    "sample_obj=data2.samples[samp1]\n",
    "data2.samples[samp1].stats.state = 2\n",
    "assemble.cluster_within.derep_and_sort(data2, sample_obj, 1, 4)\n",
    "print(sample_obj.files.edits)\n",
    "handle = sample_obj.files.edits[0]\n",
    "cmd = data2.vsearch+\\\n",
    "    \" -derep_fulllength \"+handle+\\\n",
    "    \" \"+\\\n",
    "    \" -output \"+os.path.join(data2.dirs.edits, sample_obj.name+\".derep\")+\\\n",
    "    \" -sizeout \"+\\\n",
    "    \" -threads \"+str(4)+\\\n",
    "    \" -fasta_width 0\"\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/tmp/ipyrad-test/test_edits/1A_0.fasta\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "samp1=\"1A_0\"\n",
    "sample=data2.samples[samp1]\n",
    "unmapped_fastq_handle=os.path.join(data2.dirs.edits, sample.name+\".fasta\")\n",
    "with open(os.path.realpath(unmapped_fastq_handle), 'rb') as fq:\n",
    "    quart1 = itertools.izip(*[iter(fq)]*4)\n",
    "    quarts = itertools.izip(quart1, iter(int, 1))\n",
    "    read1 = [i.strip() for i in quart[0]]\n",
    "    writing = []\n",
    "    while 1:\n",
    "        try:\n",
    "            quart = quarts.next()\n",
    "        except StopIteration:\n",
    "            break\n",
    "        read1 = [i.strip() for i in quart[0]]\n",
    "        sseq = \">\"+sample.name+\"_\"+str(0)+\\\n",
    "                           \"_c1\\n\"+read1[1]+\"\\n\"\n",
    "        writing.append(sseq)\n",
    "print(sample.files.edits[0])\n",
    "with open( sample.files.edits[0], 'w' ) as out:\n",
    "    out.write(\"\".join(writing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/tmp/ipyrad-test/test_edits/3L_0.fastq\n"
     ]
    }
   ],
   "source": [
    "samp1=\"3L_0\"\n",
    "sample=data2.samples[samp1]\n",
    "unmapped_fastq_handle=os.path.join(data2.dirs.edits, sample.name+\".fastq\")\n",
    "print(unmapped_fastq_handle)\n",
    "if True:\n",
    "    writing = []\n",
    "    with open(os.path.realpath(unmapped_fastq_handle), 'rb') as fq:\n",
    "        quart1 = itertools.izip(*[iter(fq)]*4)\n",
    "        quarts = itertools.izip(quart1, iter(int, 1))\n",
    "        writing = []\n",
    "        while 1:\n",
    "            try:\n",
    "                quart = quarts.next()\n",
    "            except StopIteration:\n",
    "                break\n",
    "            read1 = [i.strip() for i in quart[0]]\n",
    "            sseq = \">\"+sample.name+\"_\"+str(0)+\\\n",
    "                           \"_c1\\n\"+read1[1]+\"\\n\"\n",
    "            writing.append(sseq)\n",
    "\n",
    "    with open( sample.files.edits[0], 'w' ) as out:\n",
    "        out.write(\"\".join(writing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quart=[]\n",
    "quarts=[]\n",
    "quart1=[]\n",
    "nthreads=4\n",
    "preview=True\n",
    "samp1=\"1B_0\"\n",
    "sample=data1.samples[samp1]\n",
    "data=data1\n",
    "if True:\n",
    "    samhandle = os.path.join(data.dirs.edits, sample.name+\".sam\")\n",
    "    bamhandle = os.path.join(data.dirs.edits, sample.name+\".bam\")\n",
    "    unmapped_fastq_handle = os.path.join(data.dirs.edits, sample.name+\".fastq\")\n",
    "\n",
    "    ## get call string\n",
    "    cmd = data.smalt+\\\n",
    "        \" map -f sam -n \" + str(nthreads) +\\\n",
    "        \" -o \" + samhandle +\\\n",
    "        \" \" + data.get_params(28) +\\\n",
    "        \" \" + sample.files.fastq[0]\n",
    "\n",
    "    ## run smalt\n",
    "    if preview:\n",
    "        ## make this some kind of wait command that kills after a few mins\n",
    "        subprocess.call(cmd, shell=True,\n",
    "                             stderr=subprocess.STDOUT,\n",
    "                             stdout=subprocess.PIPE)\n",
    "    else:\n",
    "        subprocess.call(cmd, shell=True,\n",
    "                             stderr=subprocess.STDOUT,\n",
    "                             stdout=subprocess.PIPE)\n",
    "\n",
    "    cmd = data.samtools+\\\n",
    "        \" view -b -f 0x4 \"+samhandle+\\\n",
    "            \" > \" + bamhandle\n",
    "    subprocess.call(cmd, shell=True,\n",
    "                         stderr=subprocess.STDOUT,\n",
    "                         stdout=subprocess.PIPE)\n",
    "\n",
    "    cmd = data.samtools+\\\n",
    "        \" sort -T \"+samhandle+\".tmp\" +\\\n",
    "        \" -O bam \"+bamhandle+\\\n",
    "        \" -o \"+bamhandle+\".sorted\"\n",
    "    subprocess.call(cmd, shell=True,\n",
    "                         stderr=subprocess.STDOUT,\n",
    "                         stdout=subprocess.PIPE)\n",
    "    cmd = data.samtools+\\\n",
    "        \" bam2fq \"+bamhandle+\".sorted\"+\\\n",
    "        \" > \"+unmapped_fastq_handle\n",
    "    subprocess.call(cmd, shell=True,\n",
    "                         stderr=subprocess.STDOUT,\n",
    "                         stdout=subprocess.PIPE)\n",
    "\n",
    "    ## This is hax to get fastq to fasta to get this off the ground.\n",
    "    ## samtools bam2fq natively returns fastq, you just delete this code\n",
    "    ## when fastq pipleline is working\n",
    "    writing = []\n",
    "    with open(os.path.realpath(unmapped_fastq_handle), 'rb') as fq:\n",
    "        quart1 = itertools.izip(*[iter(fq)]*4)\n",
    "        quarts = itertools.izip(quart1, iter(int, 1))\n",
    "        writing = []\n",
    "        while 1:\n",
    "            try:\n",
    "                quart = quarts.next()\n",
    "            except StopIteration:\n",
    "                break\n",
    "            read1 = [i.strip() for i in quart[0]]\n",
    "            sseq = \">\"+sample.name+\"_\"+str(0)+\\\n",
    "                           \"_c1\\n\"+read1[1]+\"\\n\"\n",
    "            writing.append(sseq)\n",
    "\n",
    "    with open( sample.files.edits[0], 'w' ) as out:\n",
    "        out.write(\"\".join(writing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13733 + 0 in total (QC-passed reads + QC-failed reads)\n",
      "0 + 0 secondary\n",
      "0 + 0 supplementary\n",
      "0 + 0 duplicates\n",
      "0 + 0 mapped (0.00% : N/A)\n",
      "0 + 0 paired in sequencing\n",
      "0 + 0 read1\n",
      "0 + 0 read2\n",
      "0 + 0 properly paired (N/A : N/A)\n",
      "0 + 0 with itself and mate mapped\n",
      "0 + 0 singletons (N/A : N/A)\n",
      "0 + 0 with mate mapped to a different chr\n",
      "0 + 0 with mate mapped to a different chr (mapQ>=5)\n",
      "\n",
      "state                        2\n",
      "reads_raw                20099\n",
      "reads_filtered             NaN\n",
      "clusters_total             NaN\n",
      "clusters_kept              NaN\n",
      "hetero_est                 NaN\n",
      "error_est                  NaN\n",
      "reads_consens              NaN\n",
      "refseq_unmapped_reads    13733\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "cmd = data.samtools+\\\n",
    "    \" flagstat \"+bamhandle+\".sorted\"\n",
    "result = subprocess.check_output(cmd, shell=True, \n",
    "                            stderr=subprocess.STDOUT )\n",
    "print(result)\n",
    "\n",
    "#result = subprocess.check_output( cmd, shell=True, \n",
    "#                                       stderr=subprocess.STDOUT )\n",
    "sample.stats.refseq_unmapped_reads=int(result.split()[0])\n",
    "print(sample.stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1   working_directory             /tmp/ipyrad-test                             \n",
      "  2   raw_fastq_path                ./data/sim_rad_test_R1_.fastq.gz             \n",
      "  3   barcodes_path                 ./data/sim_rad_test_barcodes.txt             \n",
      "  4   sorted_fastq_path             key not recognized                           \n",
      "  5   restriction_overhang          ('TGCAG', '')                                \n",
      "  6   max_low_qual_bases            5                                            \n",
      "  7   N_processors                  4                                            \n",
      "  8   mindepth_statistical          6                                            \n",
      "  9   mindepth_majrule              6                                            \n",
      "  10  datatype                      rad                                          \n",
      "  11  clust_threshold               0.85                                         \n",
      "  12  minsamp                       4                                            \n",
      "  13  max_shared_heterozygosity     0.25                                         \n",
      "  14  prefix_outname                test-refseq                                  \n",
      "  15  phred_Qscore_offset           33                                           \n",
      "  16  max_barcode_mismatch          1                                            \n",
      "  17  filter_adapters               0                                            \n",
      "  18  filter_min_trim_len           35                                           \n",
      "  19  ploidy                        2                                            \n",
      "  20  max_stack_size                1000                                         \n",
      "  21  max_Ns_consens                5                                            \n",
      "  22  max_Hs_consens                8                                            \n",
      "  23  max_SNPs_locus                (100, 100)                                   \n",
      "  24  max_Indels_locus              (5, 99)                                      \n",
      "  25  trim_overhang                 (1, 2, 2, 1)                                 \n",
      "  26  hierarchical_clustering       0                                            \n",
      "  27  reference_sequence            /Volumes/WorkDrive/ipyrad/refhacking/MusChr1.fa\n",
      "Checking for reference sequence index. If it doesn't exist then create it.\n",
      "This could take several minutes, but it's a one time penalty, so be patient.\n",
      "Clustering 1 samples on 4 processors.\n",
      "/private/tmp/ipyrad-test/test-refseq_refmapping/1A_0-sorted-mapped.bam\n",
      ".\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>reads_raw</th>\n",
       "      <th>reads_filtered</th>\n",
       "      <th>refseq_mapped_reads</th>\n",
       "      <th>refseq_unmapped_reads</th>\n",
       "      <th>clusters_total</th>\n",
       "      <th>clusters_kept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1A_0</th>\n",
       "      <td>3</td>\n",
       "      <td>20099</td>\n",
       "      <td>20099</td>\n",
       "      <td>6297</td>\n",
       "      <td>13802</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1B_0</th>\n",
       "      <td>3</td>\n",
       "      <td>19977</td>\n",
       "      <td>19977</td>\n",
       "      <td>6244</td>\n",
       "      <td>13733</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1C_0</th>\n",
       "      <td>3</td>\n",
       "      <td>20114</td>\n",
       "      <td>20114</td>\n",
       "      <td>6280</td>\n",
       "      <td>13834</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1D_0</th>\n",
       "      <td>3</td>\n",
       "      <td>19895</td>\n",
       "      <td>19895</td>\n",
       "      <td>6263</td>\n",
       "      <td>13632</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2E_0</th>\n",
       "      <td>3</td>\n",
       "      <td>19928</td>\n",
       "      <td>19928</td>\n",
       "      <td>6055</td>\n",
       "      <td>13873</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2F_0</th>\n",
       "      <td>3</td>\n",
       "      <td>19934</td>\n",
       "      <td>19934</td>\n",
       "      <td>6171</td>\n",
       "      <td>13763</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2G_0</th>\n",
       "      <td>3</td>\n",
       "      <td>20026</td>\n",
       "      <td>20026</td>\n",
       "      <td>5980</td>\n",
       "      <td>14046</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2H_0</th>\n",
       "      <td>3</td>\n",
       "      <td>19936</td>\n",
       "      <td>19936</td>\n",
       "      <td>6052</td>\n",
       "      <td>13884</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3I_0</th>\n",
       "      <td>3</td>\n",
       "      <td>20084</td>\n",
       "      <td>20084</td>\n",
       "      <td>6391</td>\n",
       "      <td>13693</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3J_0</th>\n",
       "      <td>3</td>\n",
       "      <td>20011</td>\n",
       "      <td>20011</td>\n",
       "      <td>6190</td>\n",
       "      <td>13821</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3K_0</th>\n",
       "      <td>3</td>\n",
       "      <td>20117</td>\n",
       "      <td>20117</td>\n",
       "      <td>6320</td>\n",
       "      <td>13797</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3L_0</th>\n",
       "      <td>3</td>\n",
       "      <td>19901</td>\n",
       "      <td>19901</td>\n",
       "      <td>6268</td>\n",
       "      <td>13633</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      state  reads_raw  reads_filtered  refseq_mapped_reads  \\\n",
       "1A_0      3      20099           20099                 6297   \n",
       "1B_0      3      19977           19977                 6244   \n",
       "1C_0      3      20114           20114                 6280   \n",
       "1D_0      3      19895           19895                 6263   \n",
       "2E_0      3      19928           19928                 6055   \n",
       "2F_0      3      19934           19934                 6171   \n",
       "2G_0      3      20026           20026                 5980   \n",
       "2H_0      3      19936           19936                 6052   \n",
       "3I_0      3      20084           20084                 6391   \n",
       "3J_0      3      20011           20011                 6190   \n",
       "3K_0      3      20117           20117                 6320   \n",
       "3L_0      3      19901           19901                 6268   \n",
       "\n",
       "      refseq_unmapped_reads  clusters_total  clusters_kept  \n",
       "1A_0                  13802              25             24  \n",
       "1B_0                  13733              24             23  \n",
       "1C_0                  13834              22             22  \n",
       "1D_0                  13632              22             22  \n",
       "2E_0                  13873              23             23  \n",
       "2F_0                  13763              24             22  \n",
       "2G_0                  14046              24             24  \n",
       "2H_0                  13884              23             22  \n",
       "3I_0                  13693              23             21  \n",
       "3J_0                  13821              22             22  \n",
       "3K_0                  13797              23             23  \n",
       "3L_0                  13633              24             24  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyrad as ip\n",
    "TEST = ip.load_assembly(\"/tmp/ipyrad-test/test-refseq.assembly\")\n",
    "TEST.get_params()\n",
    "TEST.step3( [\"1A_0\"], preview=True, force=True)\n",
    "TEST.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hax for testing why clustering unmapped reads returns so few clusters???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/tmp/ipyrad-test/test-refseq_clust_0.85/1A_0.clustS.gz\n",
      "wat\n",
      "[18 21 20 18 24 23 23 20 25 20 20 23 24 17 21 20 20 19 17 18 20 18 17 23 16\n",
      " 24 24 19 20 21 22 25 20 19 22 21 12 18 24 20 20 22 20 16 16 17 18 20 19 17\n",
      " 19 19 20 20 19 18 20 21 17 19 21 23 19 19 21 17 21 28 21 20 18 19 23 18 22\n",
      " 21 23 16 17 20 16 22 15 19 17 19 20 22 22 26 19 24 24 22 13 18 20 19 21 17\n",
      " 20 19 18 17 16 22 18 19 17 22 19 22 19 18 24 20 21 15 21 16 17 21 18 15 24\n",
      " 19 21 17 20 22 18 26 23 24 22 22 21 23 21 21 26 17 22 22 18 19 25 23 19 23\n",
      " 26 18 20 20 22 24 16 23 20 24 24 21 17 20 18 21 20 22 21 20 18 20 23 19 20\n",
      " 18 21 22 22 16 22 21 22 19 20 25 16 22 22 18 24 19 17 22 23 20 21 17 21 17\n",
      " 19 22 19 14 21 19 19 17 18 17 21 24 19 22 22 20 19 21 20 19 20 17 20 18 19\n",
      " 23 23 23 15 20 22 18 16 19 22 18 23 22 18 18 22 23 24 20 20 21 18 26 19 19\n",
      " 20 19 18 22 21 17 19 22 21 20 22 18 22 25 23 16 21 18 22 21 21 21 23 19 21\n",
      " 20 22 22 17 23 23 16 19 18 23 18 20 23 19 15 25 20 24 21 24 19 18 17 18 19\n",
      " 19 21 16 18 17 19 21 17 18 24 25 17 24 18 15 22 20 21 15 23 21 23 20 19 24\n",
      " 21 20 19 20 17 24 20 19 25 23 15 21 20 23 19 24 21 23 18 21 16 21 18 15 16\n",
      " 16 24 18 16 16 21 25 24 20 24 23 23 18 23 18 21 25 24 20 12 25 23 28 19 19\n",
      " 25 20 23 21 23 18 16 21 20 21 17 21 19 23 22 24 17 28 19 17 21 18 15 21 28\n",
      " 21 16 22 21 18 16 18 18 20 22 21 18 15 20 19 17 21 22 20 21 21 20 19 21 24\n",
      " 16 24 16 19 18 16 20 18 19 22 18 21 22 22 20 15 19 24 26 26 21 22 24 21 18\n",
      " 20 18 18 22 19 21 21 19 15 17 15 17 16 22 14 20 19 20 20 25 24 22 24 20 16\n",
      " 22 21 15 23 22 21 24 15 16 15 22 17 18 23 20 23 19 18 21 20 16 19 19 22 15\n",
      " 26 24 20 21 19 25 28 16 15 22 16 23 18 21 16 22 17 21 23 19 23 18 20 20 22\n",
      " 19 15 25 21 20 23 19 19 22 23 23 17 21 21 21 21 18 21 21 22 24 16 24 22 20\n",
      " 22 24 19 15 22 22 21 19 27 14 20 17 12 15 20 18 22 23 22 20 24 19 20 23 20\n",
      " 21 20 24 19 21 20 19 24 18 23 18 23 22 15 25 18 19 18 22 23 21 22 19 19 24\n",
      " 19 20 17 19 23 21 21 23 23 20 17 22 21 27 19 17 24 24 21 18 21 17 24 22 28\n",
      " 15 19 19 16 26 12 24 21 20 18 26 19 25 17 19 20 17 19 19 25 17 21 19 17 20\n",
      " 21 23 22 20 24 23 23 21 24 16 22 21 21 17 22 23 23 23 18 30 19 20 17 18 16\n",
      " 21 11 21 21 17 26 23 17 20 17 26 19 21 20 22 21 23 19 18 24 20 22 22 23 18\n",
      " 27 20 20 26 17 15 14 18 21 23 21 23 22 19 20 20 22 22 21 25 19 24 20 22 21\n",
      " 18 22 19 19 17 23 23 20 22 23 19 20 22 20 16 18 18 17 21 21 24 24 18 21 22\n",
      " 16 18 19 20 19 24 18 17 18 19 19 17 19 15 21 20 19 18 20 16 20 20 20 21 18\n",
      " 17 23 22 21 23 16 17 16 24 18 23 19 23 21 19 19 17 21 22 25 20 20 18 18 22\n",
      " 20 25 19 23 20 19 18 16 17 24 19 19 17 22 18 14 14 23 18 19 22 16 16 19 20\n",
      " 20 21 21 21 17 17 16 20 25 20 20 20 17 17 16 19 19 17 21 25 20 17 18 20 23\n",
      " 21 20 23 22 18 15 17 22 20 16 18 14 20 16 17 16 19 19 17 17 20 23 23 24 19\n",
      " 16 21 13 21 17 19 21 14 14 23 17 15 26 19 21 16 18 21 19 22 16 19 24 23 22\n",
      " 20 22 20 23 24 16 25 20 25 18 17 20 18 17 22 20 20 17 24 16 19 19 21 21 24\n",
      " 23 20 19 21 20 19 23 22 17 19 24 20 23 23 19 17 16 19 20 20 19 22 23 17 21\n",
      " 21 24 21 14 24 24 17 20 21 21 29 21 19 21 20 20 18 21 21 20 25 17 19 25 21\n",
      " 20 19 16 16 19 19 23 18 21 23 22 20 21 15 19 19 18 20 20 17 16 19 21 18 23]\n"
     ]
    }
   ],
   "source": [
    "import gzip \n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "sample = TEST.samples[\"1A_0\"]\n",
    "data = TEST\n",
    "#ip.assemble.cluster_within.cleanup(TEST, TEST.samples[\"1A_0\"])\n",
    "if True:\n",
    "    sample.files.clusters = os.path.join(data.dirs.clusts,\n",
    "                                         sample.name+\".clustS.gz\")\n",
    "\n",
    "    print(sample.files.clusters)\n",
    "    ## get depth stats\n",
    "    infile = gzip.open(sample.files.clusters)\n",
    "    duo = itertools.izip(*[iter(infile)]*2)\n",
    "    depth = []\n",
    "    thisdepth = []\n",
    "    while 1:\n",
    "        try:\n",
    "            itera = duo.next()[0]\n",
    "            #print(itera)\n",
    "        except StopIteration:\n",
    "            print(\"wat\")\n",
    "            break\n",
    "        if itera != \"//\\n\":\n",
    "            thisdepth.append(int(itera.split(\";\")[-2][5:]))\n",
    "        else:\n",
    "            ## append and reset\n",
    "            depth.append(sum(thisdepth))\n",
    "            thisdepth = []\n",
    "    infile.close()\n",
    "\n",
    "    if depth:\n",
    "        ## make sense of stats\n",
    "        depth = np.array(depth)\n",
    "        print(depth)\n",
    "        keepmj = depth[depth >= data.paramsdict[\"mindepth_majrule\"]]\n",
    "        keepstat = depth[depth >= data.paramsdict[\"mindepth_statistical\"]]\n",
    "        ## sample assignments\n",
    "        sample.stats[\"state\"] = 3\n",
    "        sample.stats[\"clusters_total\"] = len(depth)\n",
    "        sample.stats[\"clusters_kept\"] = max([len(i) for i in \\\n",
    "                                             (keepmj, keepstat)])\n",
    "        sample.depths.total = depth\n",
    "        sample.depths.mjmin = keepmj\n",
    "        sample.depths.statmin = keepstat\n",
    "\n",
    "        data.stamp(\"s3 clustering on \"+sample.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
