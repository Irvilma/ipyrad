{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated RAD-seq data for ipyrad testing\n",
    "The simulations software simrrls is available at [github.com/dereneaton/simrrls](github.com/dereneaton/simrrls). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simrrls v.0.0.13\n",
      "ipyrad v.0.3.42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import itertools\n",
    "import numpy as np\n",
    "import simrrls\n",
    "import ipyrad as ip\n",
    "\n",
    "print \"simrrls v.{}\".format(simrrls.__version__)\n",
    "print \"ipyrad v.{}\".format(ip.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## the dir to write data to\n",
    "DIR = \"./ipsimdata/\"\n",
    "\n",
    "## if it doesn't exist make it\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "    \n",
    "## if theres anything in it, remove it\n",
    "for oldfile in glob.glob(os.path.join(DIR, \"*\")):\n",
    "    os.remove(oldfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate the data with simrrls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmin insert size allows read overlaps/adapter sequences \n",
      "\tmin insert size allows read overlaps/adapter sequences \n"
     ]
    }
   ],
   "source": [
    "%%bash -s $DIR\n",
    "\n",
    "## simulate rad_example (includes indels)\n",
    "simrrls -o $1/rad_example -f rad -dm 10 -ds 2 -I 0.05 -L 1000\n",
    "\n",
    "## simulate larger rad data set\n",
    "simrrls -o $1/rad_large -f rad -dm 10 -ds 2 -I 0.01 -L 10000\n",
    "\n",
    "## simulate pairddrad_example\n",
    "simrrls -o $1/gbs_example -f gbs -dm 10 -ds 2 -I 0.05 -L 1000\n",
    "\n",
    "## simulate pairddrad_example\n",
    "simrrls -o $1/pairddrad_example -f pairddrad -dm 10 -ds 2 -L 1000\n",
    "\n",
    "## simulate pairgbs_example\n",
    "simrrls -o $1/pairgbs_example -f pairgbs -dm 10 -ds 2 -L 1000\n",
    "\n",
    "## simulate pairddrad_wmerge_example \n",
    "simrrls -o $1/pairddrad_wmerge_example -f pairddrad -dm 10 -ds 2 \\\n",
    "                                         -i1 -50 -i2 100 -L 1000\n",
    "\n",
    "## simulate pairgbs_wmerge_example (mixture of merged and non-merged reads)\n",
    "simrrls -o $1/pairgbs_wmerge_example -f pairgbs -dm 10 -ds 2 \\\n",
    "                                       -i1 -50 -i2 100 -L 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to insert reads into simulated genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import gzip\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Utility function\n",
    "def revcomp(sequence):\n",
    "    \"returns reverse complement of a string\"\n",
    "    sequence = sequence[::-1].strip()\\\n",
    "                             .replace(\"A\", \"t\")\\\n",
    "                             .replace(\"T\", \"a\")\\\n",
    "                             .replace(\"C\", \"g\")\\\n",
    "                             .replace(\"G\", \"c\").upper()\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RAD_to_genome(R1s, R2s, n_inserts, insert_low, insert_high, out_chr):\n",
    "    \"\"\" \n",
    "    Create a simulated genome file with RAD data interspersed.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## start genome fasta sequence with 5000 random bp\n",
    "    genome = np.random.choice(list(\"ATGC\"), 5000)\n",
    "    \n",
    "    ## read in the RAD data files\n",
    "    dat1 = gzip.open(R1s, 'r')\n",
    "    qiter1 = itertools.izip(*[iter(dat1)]*4)\n",
    "    if R2s:\n",
    "        dat2 = gzip.open(R2s, 'r')\n",
    "        qiter2 = itertools.izip(*[iter(dat2)]*4)\n",
    "    else:\n",
    "        qiter2 = itertools.izip(*[iter(str, 1)]*4)\n",
    "       \n",
    "    ## sample unique reads from rads\n",
    "    uniqs = []\n",
    "    locid = 0\n",
    "    while len(uniqs) < n_inserts:\n",
    "        ## grab a read and get locus id\n",
    "        qrt1 = qiter1.next()\n",
    "        qrt2 = qiter2.next()\n",
    "        iloc = []\n",
    "        ilocid = int(qrt1[0].split(\"_\")[1][5:])\n",
    "        \n",
    "        ## go until end of locus copies\n",
    "        while ilocid == locid:\n",
    "            iloc.append([qrt1[1].strip(), qrt2[1].strip()])\n",
    "            qrt1 = qiter1.next()\n",
    "            qrt2 = qiter2.next()\n",
    "            ilocid = int(qrt1[0].split(\"_\")[1][5:])\n",
    "        \n",
    "        ## sample one read\n",
    "        uniqs.append(iloc[0])\n",
    "        locid += 1\n",
    "        \n",
    "    ## insert RADs into genome\n",
    "    for ins in range(n_inserts):   \n",
    "        ## get read, we leave the barcode on cuz it won't hurt\n",
    "        r1 = np.array(list(uniqs[ins][0]))\n",
    "        r2 = np.array(list(revcomp(uniqs[ins][1])))\n",
    "        \n",
    "        ## add R1 to genome\n",
    "        genome = np.concatenate((genome, r1), axis=0)\n",
    "        if qrt2[0]:\n",
    "            ## add insert size\n",
    "            howlong = np.random.uniform(insert_low, insert_high)\n",
    "            chunk = np.random.choice(list(\"ATGC\"), int(howlong))\n",
    "            #print howlong\n",
    "            genome = np.concatenate((genome, chunk), axis=0)\n",
    "            ## add read2\n",
    "            genome = np.concatenate((genome, r2), axis=0)\n",
    "\n",
    "        ## add spacer between loci\n",
    "        genome = np.concatenate((genome, np.random.choice(list(\"ATGC\"), 5000)), axis=0)\n",
    "\n",
    "    print(\"input genome is {} bp\".format(genome.shape[0]))\n",
    "    print(\"inserted {} loci into genome\".format(n_inserts))\n",
    "\n",
    "    with open(out_chr, \"w\") as fasta:\n",
    "        header = \">MT dna:chromosome chromosome:test:MT:1:{}:1 REF\\n\"\\\n",
    "                 .format(genome.shape[0])\n",
    "        fasta.write(header)\n",
    "        outseq = list(genome)\n",
    "        for i in range(0, len(outseq), 70):\n",
    "            fasta.write(\"{}\\n\".format(\"\".join(outseq[i:i+70])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make 'genome' data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Meta args\n",
    "N_INSERTS = 500\n",
    "INSERT_LOW = 250\n",
    "INSERT_HIGH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input genome is 2555000 bp\n",
      "inserted 500 loci into genome\n"
     ]
    }
   ],
   "source": [
    "## SE RAD data\n",
    "DATA_R1 = os.path.join(DIR, \"rad_example_R1_.fastq.gz\")\n",
    "OUTPUT_CHR = os.path.join(DIR, \"rad_example_genome.fa\")\n",
    "big = RAD_to_genome(DATA_R1, 0, N_INSERTS, \n",
    "                    INSERT_LOW, INSERT_HIGH,\n",
    "                    OUTPUT_CHR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input genome is 2555000 bp\n",
      "inserted 500 loci into genome\n"
     ]
    }
   ],
   "source": [
    "## SE GBS data\n",
    "DATA_R1 = os.path.join(DIR, \"gbs_example_R1_.fastq.gz\")\n",
    "OUTPUT_CHR = os.path.join(DIR, \"gbs_example_genome.fa\")\n",
    "RAD_to_genome(DATA_R1, 0, N_INSERTS,\n",
    "              INSERT_LOW, INSERT_HIGH, \n",
    "              OUTPUT_CHR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input genome is 2742006 bp\n",
      "inserted 500 loci into genome\n"
     ]
    }
   ],
   "source": [
    "## PAIR ddRAD data \n",
    "DATA_R1 = os.path.join(DIR, \"pairddrad_example_R1_.fastq.gz\")\n",
    "DATA_R2 = os.path.join(DIR, \"pairddrad_example_R2_.fastq.gz\")\n",
    "OUTPUT_CHR = os.path.join(DIR, \"pairddrad_example_genome.fa\")\n",
    "RAD_to_genome(DATA_R1, DATA_R2, N_INSERTS, \n",
    "              INSERT_LOW, INSERT_HIGH,\n",
    "              OUTPUT_CHR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input genome is 2742014 bp\n",
      "inserted 500 loci into genome\n"
     ]
    }
   ],
   "source": [
    "## PAIR ddRAD data \n",
    "DATA_R1 = os.path.join(DIR, \"pairddrad_wmerge_example_R1_.fastq.gz\")\n",
    "DATA_R2 = os.path.join(DIR, \"pairddrad_wmerge_example_R2_.fastq.gz\")\n",
    "OUTPUT_CHR = os.path.join(DIR, \"pairddrad_wmerge_example_genome.fa\")\n",
    "RAD_to_genome(DATA_R1, DATA_R2, N_INSERTS, \n",
    "              INSERT_LOW, INSERT_HIGH,  \n",
    "              OUTPUT_CHR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input genome is 2742456 bp\n",
      "inserted 500 loci into genome\n"
     ]
    }
   ],
   "source": [
    "## PAIR GBS data\n",
    "DATA_R1 = os.path.join(DIR, \"pairgbs_wmerge_example_R1_.fastq.gz\")\n",
    "DATA_R2 = os.path.join(DIR, \"pairgbs_wmerge_example_R2_.fastq.gz\")\n",
    "OUTPUT_CHR = os.path.join(DIR, \"pairgbs_wmerge_example_genome.fa\")\n",
    "RAD_to_genome(DATA_R1, DATA_R2, N_INSERTS, \n",
    "              INSERT_LOW, INSERT_HIGH, \n",
    "              OUTPUT_CHR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run tests in ipyrad\n",
    "### rad_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  New Assembly: denovo\n",
      "\n",
      "  Assembly: denovo\n",
      "\n",
      "  [####################] 100%  chunking large files  | 0:00:00 \n",
      "  [####################] 100%  sorting reads         | 0:00:04 \n",
      "  [####################] 100%  writing/compressing   | 0:00:01 \n",
      "\n",
      "  [####################] 100%  processing reads      | 0:00:33 \n",
      "\n",
      "  [####################] 100%  dereplicating         | 0:00:00 \n",
      "  [####################] 100%  clustering            | 0:00:00 \n",
      "  [####################] 100%  building clusters     | 0:00:00 \n",
      "  [####################] 100%  chunking              | 0:00:00 \n",
      "  [####################] 100%  aligning              | 0:00:33 \n",
      "  [####################] 100%  concatenating         | 0:00:00 \n",
      "\n",
      "  [####################] 100%  inferring [H, E]      | 0:00:51 \n",
      "\n",
      "  [####################] 100%  consensus calling     | 0:00:24 \n",
      "  [####################] 100%  concat/shuffle input  | 0:00:00 \n",
      "  [####################] 100%  clustering across     | 0:00:00 \n",
      "  [####################] 100%  building clusters     | 0:00:00 \n",
      "  [####################] 100%  aligning clusters     | 0:00:07 \n",
      "  [####################] 100%  indexing clusters     | 0:00:05 \n",
      "  [####################] 100%  building database     | 0:00:04 \n",
      "  [####################] 100%  filtering loci        | 0:00:04 \n",
      "  [####################] 100%  building loci/stats   | 0:00:01 \n",
      "  [####################] 100%  building vcf file     | 0:00:14 \n",
      "  [####################] 100%  writing outfiles      | 0:00:01 \n",
      "  Outfiles written to: ~/Documents/ipyrad/tests/tests1/denovo_outfiles\n",
      "\n",
      "  Assembly: reference\n",
      "\n",
      "  [####################] 100%  chunking large files  | 0:00:00 \n",
      "  [####################] 100%  sorting reads         | 0:00:03 \n",
      "  [####################] 100%  writing/compressing   | 0:00:00 \n",
      "\n",
      "  [####################] 100%  processing reads      | 0:00:32 \n",
      "\n",
      "  [####################] 100%  dereplicating         | 0:00:00 \n",
      "  [####################] 100%  mapping               | 0:00:52 \n",
      "  [####################] 100%  finalize mapping      | 0:00:12 \n",
      "  [####################] 100%  chunking              | 0:00:00 \n",
      "  [####################] 100%  aligning              | 0:00:30 \n",
      "  [####################] 100%  concatenating         | 0:00:00 \n",
      "\n",
      "  [####################] 100%  inferring [H, E]      | 0:00:36 \n",
      "\n",
      "  [####################] 100%  consensus calling     | 0:00:14 \n",
      "  [####################] 100%  concat/shuffle input  | 0:00:00 \n",
      "  [####################] 100%  clustering across     | 0:00:00 \n",
      "  [####################] 100%  building clusters     | 0:00:00 \n",
      "  [####################] 100%  aligning clusters     | 0:00:03 \n",
      "  [####################] 100%  indexing clusters     | 0:00:03 \n",
      "  [####################] 100%  building database     | 0:00:02 \n",
      "  [####################] 100%  filtering loci        | 0:00:00 \n",
      "  [####################] 100%  building loci/stats   | 0:00:01 \n",
      "  [####################] 100%  building vcf file     | 0:00:07 \n",
      "  [####################] 100%  writing outfiles      | 0:00:01 \n",
      "  Outfiles written to: ~/Documents/ipyrad/tests/tests1/reference_outfiles\n",
      "\n",
      "  Assembly: denovo-plus\n",
      "\n",
      "  [####################] 100%  chunking large files  | 0:00:00 \n",
      "  [####################] 100%  sorting reads         | 0:00:03 \n",
      "  [####################] 100%  writing/compressing   | 0:00:00 \n",
      "\n",
      "  [####################] 100%  processing reads      | 0:00:33 \n",
      "\n",
      "\n",
      "  Encountered an unexpected error (see ./ipyrad_log.txt)\n",
      "  Error message is below -------------------------------\n",
      "    Assembly method denovo+reference requires that you enter a 'reference_sequence_path'.\n",
      "    \n",
      "\n",
      "  Assembly: denovo-minus\n",
      "\n",
      "  [####################] 100%  chunking large files  | 0:00:00 \n",
      "  [####################] 100%  sorting reads         | 0:00:03 \n",
      "  [####################] 100%  writing/compressing   | 0:00:00 \n",
      "\n",
      "  [####################] 100%  processing reads      | 0:00:33 \n",
      "\n",
      "\n",
      "  Encountered an unexpected error (see ./ipyrad_log.txt)\n",
      "  Error message is below -------------------------------\n",
      "    Assembly method denovo-reference requires that you enter a 'reference_sequence_path'.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "## create an assembly for denovo\n",
    "data1 = ip.Assembly(\"denovo\")\n",
    "data1.set_params(1, \"tests1\")\n",
    "data1.set_params(2, os.path.join(DIR, 'rad_example_R1_.fastq.gz'))\n",
    "data1.set_params(3, os.path.join(DIR, 'rad_example_barcodes.txt'))\n",
    "\n",
    "## branch into an assembly for reference\n",
    "data2 = data1.branch(\"reference\")\n",
    "data2.set_params(5, 'reference')\n",
    "data2.set_params(6, os.path.join(DIR, 'rad_example_genome.fa'))\n",
    "\n",
    "## branch into an assembly for denovo+reference\n",
    "data3 = data2.branch(\"denovo-plus\")\n",
    "data3.set_params(5, 'denovo+reference')\n",
    "\n",
    "## branch into an assembly for reference\n",
    "data4 = data2.branch(\"denovo-minus\")\n",
    "data4.set_params(5, 'denovo-reference')\n",
    "\n",
    "## assemble both\n",
    "data1.run(\"1234567\", force=True)\n",
    "data2.run(\"1234567\", force=True)\n",
    "data3.run(\"1234567\", force=True)\n",
    "data4.run(\"1234567\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'clusters_hidepth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-cce3ab63dc2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_dfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms7_loci\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_coverage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mN_INSERTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclusters_hidepth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdata3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_dfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms7_loci\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_coverage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deren/anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2670\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'clusters_hidepth'"
     ]
    }
   ],
   "source": [
    "## check results\n",
    "assert all(data1.stats.clusters_hidepth == 1000)\n",
    "assert data1.stats_dfs.s7_loci.sum_coverage.max() == 1000\n",
    "\n",
    "assert all(data2.stats.clusters_hidepth == N_INSERTS)\n",
    "assert data2.stats_dfs.s7_loci.sum_coverage.max() == N_INSERTS\n",
    "\n",
    "assert all(data3.stats.clusters_hidepth == 1000)\n",
    "assert data3.stats_dfs.s7_loci.sum_coverage.max() == 1000\n",
    "\n",
    "assert all(data4.stats.clusters_hidepth == 1000-N_INSERTS)\n",
    "assert data4.stats_dfs.s7_loci.sum_coverage.max() == 1000-N_INSERTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gbs example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  New Assembly: denovo\n",
      "\n",
      "  Assembly: denovo\n",
      "\n",
      "  [####################] 100%  chunking large files  | 0:00:00 \n",
      "  [####################] 100%  sorting reads         | 0:00:03 \n",
      "  [####################] 100%  writing/compressing   | 0:00:00 \n",
      "\n",
      "  [####################] 100%  processing reads      | 0:00:33 \n",
      "\n",
      "  [####################] 100%  dereplicating         | 0:00:00 \n",
      "  [####################] 100%  clustering            | 0:00:00 \n",
      "  [####################] 100%  building clusters     | 0:00:00 \n",
      "  [####################] 100%  chunking              | 0:00:00 \n",
      "  [####################] 100%  aligning              | 0:00:51 \n",
      "  [####################] 100%  concatenating         | 0:00:00 \n",
      "\n",
      "  [####################] 100%  inferring [H, E]      | 0:00:51 \n",
      "\n",
      "  [####################] 100%  consensus calling     | 0:00:24 \n",
      "  [####################] 100%  concat/shuffle input  | 0:00:00 \n",
      "  [####################] 100%  clustering across     | 0:00:00 \n",
      "  [####################] 100%  building clusters     | 0:00:00 \n",
      "  [####################] 100%  aligning clusters     | 0:00:08 \n",
      "  [####################] 100%  indexing clusters     | 0:00:03 \n",
      "  [####################] 100%  building database     | 0:00:05 \n",
      "  [####################] 100%  filtering loci        | 0:00:04 \n",
      "  [####################] 100%  building loci/stats   | 0:00:01 \n",
      "  [####################] 100%  building vcf file     | 0:00:14 \n",
      "  [####################] 100%  writing outfiles      | 0:00:01 \n",
      "  Outfiles written to: ~/Documents/ipyrad/tests/tests2/denovo_outfiles\n",
      "\n",
      "  Assembly: reference\n",
      "\n",
      "  [####################] 100%  chunking large files  | 0:00:00 \n",
      "  [####################] 100%  sorting reads         | 0:00:03 \n",
      "  [####################] 100%  writing/compressing   | 0:00:00 \n",
      "\n",
      "  [####################] 100%  processing reads      | 0:00:32 \n",
      "\n",
      "  [####################] 100%  dereplicating         | 0:00:00 \n",
      "  [####################] 100%  mapping               | 0:00:55 \n",
      "  [####################] 100%  finalize mapping      | 0:00:20 \n",
      "  [####################] 100%  chunking              | 0:00:00 \n",
      "  [####################] 100%  aligning              | 0:00:37 \n",
      "  [####################] 100%  concatenating         | 0:00:00 \n",
      "\n",
      "  [####################] 100%  inferring [H, E]      | 0:00:37 \n",
      "\n",
      "  [####################] 100%  consensus calling     | 0:00:14 \n",
      "  [####################] 100%  concat/shuffle input  | 0:00:00 \n",
      "  [####################] 100%  clustering across     | 0:00:00 \n",
      "  [####################] 100%  building clusters     | 0:00:00 \n",
      "  [####################] 100%  aligning clusters     | 0:00:04 \n",
      "  [####################] 100%  indexing clusters     | 0:00:03 \n",
      "  [####################] 100%  building database     | 0:00:02 \n",
      "  [####################] 100%  filtering loci        | 0:00:00 \n",
      "  [####################] 100%  building loci/stats   | 0:00:01 \n",
      "  [####################] 100%  building vcf file     | 0:00:07 \n",
      "  [####################] 100%  writing outfiles      | 0:00:01 \n",
      "  Outfiles written to: ~/Documents/ipyrad/tests/tests2/reference_outfiles\n",
      "\n",
      "  Assembly: denovo-plus\n",
      "\n",
      "  [####################] 100%  chunking large files  | 0:00:00 \n",
      "  [####################] 100%  sorting reads         | 0:00:03 \n",
      "  [####################] 100%  writing/compressing   | 0:00:00 \n",
      "\n",
      "  [####################] 100%  processing reads      | 0:00:34 \n",
      "\n",
      "    Force reindexing of reference sequence\n",
      "  [####################] 100%  dereplicating         | 0:00:00 \n",
      "  [####################] 100%  mapping               | 0:00:54 \n",
      "  [####################] 100%  clustering            | 0:00:00 \n",
      "  [####################] 100%  building clusters     | 0:00:00 \n",
      "  [####################] 100%  finalize mapping      | 0:00:21 \n",
      "  [####################] 100%  chunking              | 0:00:00 \n",
      "  [####################] 100%  aligning              | 0:01:06 \n",
      "  [####################] 100%  concatenating         | 0:00:00 \n",
      "\n",
      "  [####################] 100%  inferring [H, E]      | 0:00:55 \n",
      "\n",
      "  [####################] 100%  consensus calling     | 0:00:26 \n",
      "  [####################] 100%  concat/shuffle input  | 0:00:00 \n",
      "  [####################] 100%  clustering across     | 0:00:00 \n",
      "  [####################] 100%  building clusters     | 0:00:00 \n",
      "  [####################] 100%  aligning clusters     | 0:00:10 \n",
      "  [####################] 100%  indexing clusters     | 0:00:03 \n",
      "  [####################] 100%  building database     | 0:00:05 \n",
      "  [####################] 100%  filtering loci        | 0:00:00 \n",
      "  [####################] 100%  building loci/stats   | 0:00:01 \n",
      "  [####################] 100%  building vcf file     | 0:00:14 \n",
      "  [####################] 100%  writing outfiles      | 0:00:01 \n",
      "  Outfiles written to: ~/Documents/ipyrad/tests/tests2/denovo-plus_outfiles\n",
      "\n",
      "  Assembly: denovo-minus\n",
      "\n",
      "  [####################] 100%  chunking large files  | 0:00:00 \n",
      "  [####################] 100%  sorting reads         | 0:00:03 \n",
      "  [####################] 100%  writing/compressing   | 0:00:00 \n",
      "\n",
      "  [####################] 100%  processing reads      | 0:00:35 \n",
      "\n",
      "    Force reindexing of reference sequence\n",
      "  [####################] 100%  dereplicating         | 0:00:00 \n",
      "  [####################] 100%  mapping               | 0:00:11 \n",
      "  [####################] 100%  clustering            | 0:00:00 \n",
      "  [####################] 100%  building clusters     | 0:00:00 \n",
      "  [####################] 100%  chunking              | 0:00:00 \n",
      "  [####################] 100%  aligning              | 0:00:27 \n",
      "  [####################] 100%  concatenating         | 0:00:00 \n",
      "\n",
      "  [####################] 100%  inferring [H, E]      | 0:00:37 \n",
      "\n",
      "  [####################] 100%  consensus calling     | 0:00:15 \n",
      "  [####################] 100%  concat/shuffle input  | 0:00:00 \n",
      "  [####################] 100%  clustering across     | 0:00:00 \n",
      "  [####################] 100%  building clusters     | 0:00:00 \n",
      "  [####################] 100%  aligning clusters     | 0:00:04 \n",
      "  [####################] 100%  indexing clusters     | 0:00:03 \n",
      "  [####################] 100%  building database     | 0:00:02 \n",
      "  [####################] 100%  filtering loci        | 0:00:04 \n",
      "  [####################] 100%  building loci/stats   | 0:00:01 \n",
      "  [####################] 100%  building vcf file     | 0:00:07 \n",
      "  [####################] 100%  writing outfiles      | 0:00:01 \n",
      "  Outfiles written to: ~/Documents/ipyrad/tests/tests2/denovo-minus_outfiles\n"
     ]
    }
   ],
   "source": [
    "## create an assembly for denovo\n",
    "data1 = ip.Assembly(\"denovo\")\n",
    "data1.set_params(1, \"tests2\")\n",
    "data1.set_params(2, os.path.join(DIR, \"gbs_example_R1_.fastq.gz\"))\n",
    "data1.set_params(3, os.path.join(DIR, \"gbs_example_barcodes.txt\"))\n",
    "\n",
    "## branch into an assembly for reference\n",
    "data2 = data1.branch(\"reference\")\n",
    "data2.set_params(5, \"reference\")\n",
    "data2.set_params(6, os.path.join(DIR, \"gbs_example_genome.fa\"))\n",
    "\n",
    "## branch into an assembly for denovo+reference\n",
    "data3 = data2.branch(\"denovo-plus\")\n",
    "data3.set_params(5, 'denovo+reference')\n",
    "\n",
    "## branch into an assembly for reference\n",
    "data4 = data2.branch(\"denovo-minus\")\n",
    "data4.set_params(5, 'denovo-reference')\n",
    "\n",
    "## assemble both\n",
    "data1.run(\"1234567\", force=True)\n",
    "data2.run(\"1234567\", force=True)\n",
    "data3.run(\"1234567\", force=True)\n",
    "data4.run(\"1234567\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## check results\n",
    "assert all(data1.stats.clusters_hidepth == 1000)\n",
    "assert data1.stats_dfs.s7_loci.sum_coverage.max() == 1000\n",
    "\n",
    "assert all(data2.stats.clusters_hidepth == N_INSERTS)\n",
    "assert data2.stats_dfs.s7_loci.sum_coverage.max() == N_INSERTS\n",
    "\n",
    "assert all(data3.stats.clusters_hidepth == 1000)\n",
    "assert data3.stats_dfs.s7_loci.sum_coverage.max() == 1000\n",
    "\n",
    "assert all(data4.stats.clusters_hidepth == 1000-N_INSERTS)\n",
    "assert data4.stats_dfs.s7_loci.sum_coverage.max() == 1000-N_INSERTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pairddrad without merged reads example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create an assembly for denovo\n",
    "data1 = ip.Assembly(\"denovo\")\n",
    "data1.set_params(1, \"tests3\")\n",
    "data1.set_params(2, os.path.join(DIR, \"pairddrad_example_*.fastq.gz\"))\n",
    "data1.set_params(3, os.path.join(DIR, \"pairddrad_example_barcodes.txt\"))\n",
    "data1.set_params(7, \"pairddrad\")\n",
    "data1.set_params(8, (\"TGCAG\", \"CCGG\"))\n",
    "\n",
    "## branch into an assembly for reference\n",
    "data2 = data1.branch(\"reference\")\n",
    "data2.set_params(5, \"reference\")\n",
    "data2.set_params(6, os.path.join(DIR, \"pairddrad_example_genome.fa\"))\n",
    "\n",
    "## branch into an assembly for denovo+reference\n",
    "data3 = data1.branch(\"denovo-plus\")\n",
    "data3.set_params(5, 'denovo+reference')\n",
    "\n",
    "## branch into an assembly for reference\n",
    "data4 = data1.branch(\"denovo-minus\")\n",
    "data4.set_params(5, 'denovo-reference')\n",
    "\n",
    "## assemble both\n",
    "data1.run(\"1234567\", force=True)\n",
    "data2.run(\"1234567\", force=True)\n",
    "data3.run(\"1234567\", force=True)\n",
    "data4.run(\"1234567\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## check results\n",
    "assert all(data1.stats.clusters_hidepth == 1000)\n",
    "assert data1.stats_dfs.s7_loci.sum_coverage.max() == 1000\n",
    "\n",
    "assert all(data2.stats.clusters_hidepth == N_INSERTS)\n",
    "assert data2.stats_dfs.s7_loci.sum_coverage.max() == N_INSERTS\n",
    "\n",
    "assert all(data3.stats.clusters_hidepth == 1000)\n",
    "assert data3.stats_dfs.s7_loci.sum_coverage.max() == 1000\n",
    "\n",
    "assert all(data4.stats.clusters_hidepth == 1000-N_INSERTS)\n",
    "assert data4.stats_dfs.s7_loci.sum_coverage.max() == 1000-N_INSERTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pairgbs w/ merged reads example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create an assembly for denovo\n",
    "data1 = ip.Assembly(\"denovo\")\n",
    "data1.set_params(1, \"tests4\")\n",
    "data1.set_params(2, os.path.join(DIR, \"pairgbs_wmerge_example_*.fastq.gz\"))\n",
    "data1.set_params(3, os.path.join(DIR, \"pairgbs_wmerge_example_barcodes.txt\"))\n",
    "data1.set_params(7, \"pairgbs\")\n",
    "data1.set_params(8, (\"TGCAG\", \"TGCAG\"))\n",
    "\n",
    "##...\n",
    "\n",
    "\n",
    "## branch into an assembly for denovo+reference\n",
    "data3 = data1.branch(\"denovo-plus\")\n",
    "data3.set_params(5, 'denovo+reference')\n",
    "\n",
    "## branch into an assembly for reference\n",
    "data4 = data1.branch(\"denovo-minus\")\n",
    "data4.set_params(5, 'denovo-reference')\n",
    "\n",
    "## assemble both\n",
    "data1.run(\"1234567\", force=True)\n",
    "data2.run(\"1234567\", force=True)\n",
    "data3.run(\"1234567\", force=True)\n",
    "data4.run(\"1234567\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## check results\n",
    "assert all(data1.stats.clusters_hidepth == 1000)\n",
    "assert data1.stats_dfs.s7_loci.sum_coverage.max() == 1000\n",
    "\n",
    "assert all(data2.stats.clusters_hidepth == N_INSERTS)\n",
    "assert data2.stats_dfs.s7_loci.sum_coverage.max() == N_INSERTS\n",
    "\n",
    "assert all(data3.stats.clusters_hidepth == 1000)\n",
    "assert data3.stats_dfs.s7_loci.sum_coverage.max() == 1000\n",
    "\n",
    "assert all(data4.stats.clusters_hidepth == 1000-N_INSERTS)\n",
    "assert data4.stats_dfs.s7_loci.sum_coverage.max() == 1000-N_INSERTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create zipped archive of sim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ipsimdata/\n",
      "./ipsimdata/pairgbs_example_R2_.fastq.gz\n",
      "./ipsimdata/pairgbs_wmerge_example_barcodes.txt\n",
      "./ipsimdata/rad_example_genome.fa\n",
      "./ipsimdata/pairddrad_example_genome.fa\n",
      "./ipsimdata/pairgbs_example_R1_.fastq.gz\n",
      "./ipsimdata/pairgbs_wmerge_example_R2_.fastq.gz\n",
      "./ipsimdata/pairddrad_example_R2_.fastq.gz\n",
      "./ipsimdata/pairgbs_wmerge_example_genome.fa\n",
      "./ipsimdata/pairddrad_wmerge_example_genome.fa\n",
      "./ipsimdata/pairgbs_wmerge_example_R1_.fastq.gz\n",
      "./ipsimdata/gbs_example_barcodes.txt\n",
      "./ipsimdata/pairgbs_example_barcodes.txt\n",
      "./ipsimdata/pairddrad_example_R1_.fastq.gz\n",
      "./ipsimdata/pairddrad_wmerge_example_barcodes.txt\n",
      "./ipsimdata/rad_example_barcodes.txt\n",
      "./ipsimdata/pairddrad_wmerge_example_R1_.fastq.gz\n",
      "./ipsimdata/pairddrad_wmerge_example_R2_.fastq.gz\n",
      "./ipsimdata/gbs_example_R1_.fastq.gz\n",
      "./ipsimdata/pairddrad_example_barcodes.txt\n",
      "./ipsimdata/rad_example_R1_.fastq.gz\n",
      "./ipsimdata/gbs_example_genome.fa\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $DIR\n",
    "## compressed dir/ w/ all data files\n",
    "tar -zcvf ipsimdata.tar.gz $1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
